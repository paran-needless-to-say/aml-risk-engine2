"""
ì‹¤ì œ AML ì‹œìŠ¤í…œ ê²°ê³¼ ë°ëª¨
ì´ í”„ë¡œì íŠ¸ë¡œ ë§Œë“  AMLì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ ë³´ì—¬ì¤Œ

Usage:
    python aml_demo.py
        â†’ ì‚¬ì „ ì •ì˜ëœ 3ê°€ì§€ ì˜ˆì‹œ ê²°ê³¼ë¥¼ ì¶œë ¥ (ê¸°ì¡´ ë°ëª¨ ëª¨ë“œ)

    python aml_demo.py --chain <bsc|ethereum|polygon> --contract <contract_address>
        â†’ data/transactions/<chain>/<contract>.csv ë¥¼ ë¡œë“œí•´ ì‹¤ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„
"""

import argparse
from datetime import datetime
from pathlib import Path

import numpy as np
import pandas as pd
import networkx as nx
from networkx.algorithms import approximation as nx_approx

class ExplainableAMLDetector:
    """
    ì„¤ëª… ê°€ëŠ¥í•œ AML íƒì§€ ì‹œìŠ¤í…œ
    """
    
    def __init__(self):
        # ì„ê³„ê°’ ì„¤ì • (ì‹¤ì œ ë°ì´í„° ë¶„ì„ í›„ ì„¤ì •)
        self.thresholds = {
            'density_high': 0.8,
            'density_low': 0.01,
            'reciprocity_low': 0.1,
            'assortativity_neg': -0.3,
            'clustering_low': 0.05,
            'diameter_high': 15,
            'nodes_high': 10000,
            'edges_ratio_high': 3
        }
        
        # ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜
        self.scenarios = {
            'S1_LAYERING': {
                'name': 'ë ˆì´ì–´ë§ (Layering)',
                'description': 'ìê¸ˆì„ ì—¬ëŸ¬ ê³„ì¢Œë¡œ ë³µì¡í•˜ê²Œ ì´ë™ì‹œì¼œ ì¶œì²˜ ì€í',
                'pattern': ['density_high', 'diameter_high', 'clustering_low'],
                'severity': 'HIGH',
                'sar_required': True  # SAR ì œì¶œ í•„ìš”
            },
            'S2_SMURFING': {
                'name': 'ìŠ¤ë¨¸í•‘ (Smurfing)',
                'description': 'ì†Œì•¡ì„ ë‹¤ìˆ˜ ê³„ì¢Œë¡œ ë¶„ì‚°í•˜ì—¬ íƒì§€ íšŒí”¼',
                'pattern': ['nodes_high', 'edges_ratio_high', 'reciprocity_low'],
                'severity': 'HIGH',
                'sar_required': True
            },
            'S3_RAPID_MOVEMENT': {
                'name': 'ê¸‰ì† ìê¸ˆ ì´ë™',
                'description': 'ì§§ì€ ì‹œê°„ì— ì—¬ëŸ¬ ê³„ì¢Œë¡œ ë¹ ë¥¸ ìê¸ˆ íë¦„',
                'pattern': ['reciprocity_low', 'diameter_high'],
                'severity': 'MEDIUM',
                'sar_required': False
            },
            'S4_MIXER_PATTERN': {
                'name': 'ë¯¹ì„œ/í…€ë¸”ëŸ¬ ì‚¬ìš©',
                'description': 'ì•”í˜¸í™”í ë¯¹ì‹± ì„œë¹„ìŠ¤ ì‚¬ìš© ì˜ì‹¬',
                'pattern': ['assortativity_neg', 'clustering_low', 'nodes_high'],
                'severity': 'HIGH',
                'sar_required': True
            },
            'S5_PUMP_DUMP': {
                'name': 'Pump & Dump',
                'description': 'ì‹œì„¸ ì¡°ì‘ ì˜ì‹¬ íŒ¨í„´',
                'pattern': ['density_low', 'nodes_high', 'edges_ratio_high'],
                'severity': 'MEDIUM',
                'sar_required': False
            }
        }
    
    def analyze_token(self, token_address, graph_metrics, transaction_history=None):
        """
        í† í° ë¶„ì„ ë° ë¦¬ìŠ¤í¬ í‰ê°€
        
        Returns: ì™„ì „í•œ AML ë³´ê³ ì„œ
        """
        
        # 1. í”¼ì²˜ ì´ìƒ íƒì§€
        anomalies = self._check_feature_anomalies(graph_metrics)
        
        # 2. ì‹œë‚˜ë¦¬ì˜¤ ë§¤ì¹­
        matched_scenarios = self._match_scenarios(anomalies)
        
        # 3. AI ëª¨ë¸ ì ìˆ˜ (ì‹œë®¬ë ˆì´ì…˜)
        ai_score = self._simulate_ai_model(graph_metrics)
        
        # 4. ìµœì¢… ë¦¬ìŠ¤í¬ ì ìˆ˜ ê³„ì‚°
        risk_score = self._calculate_final_risk_score(matched_scenarios, ai_score)
        
        # 5. ì„¤ëª… ìƒì„±
        explanation = self._generate_detailed_explanation(
            graph_metrics, anomalies, matched_scenarios, ai_score, risk_score
        )
        
        # 6. ê¶Œì¥ ì¡°ì¹˜
        recommendations = self._generate_recommendations(risk_score, matched_scenarios)
        
        # 7. ì´ìœ  ì½”ë“œ ìƒì„±
        reason_code = self._generate_reason_code(matched_scenarios, anomalies)
        
        return {
            'token_address': token_address,
            'analysis_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'risk_score': risk_score,
            'risk_level': self._get_risk_level(risk_score),
            'ai_model_score': ai_score,
            'matched_scenarios': matched_scenarios,
            'anomalous_features': anomalies,
            'explanation': explanation,
            'reason_code': reason_code,
            'recommendations': recommendations,
            'requires_sar': any(s.get('sar_required', False) for s in matched_scenarios),
            'graph_metrics': graph_metrics
        }
    
    def _check_feature_anomalies(self, metrics):
        """í”¼ì²˜ ì´ìƒ íƒì§€"""
        anomalies = []
        
        if metrics['Density'] > self.thresholds['density_high']:
            anomalies.append('density_high')
        elif metrics['Density'] < self.thresholds['density_low']:
            anomalies.append('density_low')
        
        if metrics['Reciprocity'] < self.thresholds['reciprocity_low']:
            anomalies.append('reciprocity_low')
        
        if metrics['Assortativity'] < self.thresholds['assortativity_neg']:
            anomalies.append('assortativity_neg')
        
        if metrics['Clustering_Coefficient'] < self.thresholds['clustering_low']:
            anomalies.append('clustering_low')
        
        if metrics['Effective_Diameter'] > self.thresholds['diameter_high']:
            anomalies.append('diameter_high')
        
        if metrics['Num_nodes'] > self.thresholds['nodes_high']:
            anomalies.append('nodes_high')
        
        edge_ratio = metrics['Num_edges'] / max(metrics['Num_nodes'], 1)
        if edge_ratio > self.thresholds['edges_ratio_high']:
            anomalies.append('edges_ratio_high')
        
        return anomalies
    
    def _match_scenarios(self, anomalies):
        """ì‹œë‚˜ë¦¬ì˜¤ ë§¤ì¹­"""
        matched = []
        anomaly_set = set(anomalies)
        
        for scenario_id, scenario_data in self.scenarios.items():
            pattern_set = set(scenario_data['pattern'])
            match_count = len(anomaly_set & pattern_set)
            match_ratio = match_count / len(pattern_set)
            
            if match_ratio >= 0.5:  # 50% ì´ìƒ ë§¤ì¹­
                matched.append({
                    'id': scenario_id,
                    'name': scenario_data['name'],
                    'description': scenario_data['description'],
                    'severity': scenario_data['severity'],
                    'confidence': match_ratio * 100,
                    'matched_features': list(anomaly_set & pattern_set),
                    'sar_required': scenario_data.get('sar_required', False)
                })
        
        matched.sort(key=lambda x: x['confidence'], reverse=True)
        return matched
    
    def _simulate_ai_model(self, metrics):
        """
        AI ëª¨ë¸ ì ìˆ˜ ì‹œë®¬ë ˆì´ì…˜
        ì‹¤ì œë¡œëŠ” í•™ìŠµëœ DOMINANT, GAE ë“±ì˜ ëª¨ë¸ ì‚¬ìš©
        """
        # ì—¬ëŸ¬ í”¼ì²˜ë¥¼ ì¢…í•©í•˜ì—¬ ì´ìƒ ì ìˆ˜ ê³„ì‚°
        score = 0
        
        # ë°€ì§‘ë„ ê¸°ë°˜
        if metrics['Density'] > 0.7 or metrics['Density'] < 0.02:
            score += 0.25
        
        # ë„¤íŠ¸ì›Œí¬ êµ¬ì¡° ê¸°ë°˜
        if metrics['Assortativity'] < -0.2:
            score += 0.2
        
        # ê±°ë˜ íŒ¨í„´ ê¸°ë°˜
        if metrics['Reciprocity'] < 0.15:
            score += 0.25
        
        # ê·œëª¨ ê¸°ë°˜
        if metrics['Num_nodes'] > 5000:
            score += 0.15
        
        # í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜
        if metrics['Clustering_Coefficient'] < 0.1:
            score += 0.15
        
        return min(score, 1.0)
    
    def _calculate_final_risk_score(self, scenarios, ai_score):
        """ìµœì¢… ë¦¬ìŠ¤í¬ ì ìˆ˜ (0-100)"""
        
        # AI ëª¨ë¸ ì ìˆ˜ (50%)
        score = ai_score * 50
        
        # ì‹œë‚˜ë¦¬ì˜¤ ë§¤ì¹­ (50%)
        if scenarios:
            severity_weights = {'HIGH': 50, 'MEDIUM': 35, 'LOW': 20}
            scenario_score = 0
            
            for scenario in scenarios:
                weight = severity_weights.get(scenario['severity'], 20)
                scenario_score += weight * (scenario['confidence'] / 100)
            
            score += min(scenario_score, 50)
        
        return min(score, 100)
    
    def _get_risk_level(self, score):
        """ë¦¬ìŠ¤í¬ ë ˆë²¨"""
        if score >= 75:
            return 'CRITICAL'
        elif score >= 50:
            return 'HIGH'
        elif score >= 30:
            return 'MEDIUM'
        else:
            return 'LOW'
    
    def _generate_detailed_explanation(self, metrics, anomalies, scenarios, ai_score, risk_score):
        """ìƒì„¸ ì„¤ëª…"""
        lines = []
        
        lines.append("=" * 70)
        lines.append("ğŸ” AML ë¶„ì„ ê²°ê³¼ ë³´ê³ ì„œ")
        lines.append("=" * 70)
        
        lines.append(f"\nğŸ“Š **ì¢…í•© ë¦¬ìŠ¤í¬ í‰ê°€**")
        lines.append(f"  â€¢ ìµœì¢… ë¦¬ìŠ¤í¬ ì ìˆ˜: {risk_score:.1f}/100")
        lines.append(f"  â€¢ ë¦¬ìŠ¤í¬ ë ˆë²¨: {self._get_risk_level(risk_score)}")
        lines.append(f"  â€¢ AI ëª¨ë¸ ì´ìƒ ì ìˆ˜: {ai_score:.2f}")
        
        if scenarios:
            lines.append(f"\nâš ï¸  **íƒì§€ëœ ì˜ì‹¬ ì‹œë‚˜ë¦¬ì˜¤ ({len(scenarios)}ê±´)**")
            for i, scenario in enumerate(scenarios[:3], 1):
                lines.append(f"\n  [{i}] {scenario['name']}")
                lines.append(f"      ì‹ ë¢°ë„: {scenario['confidence']:.0f}%")
                lines.append(f"      ì‹¬ê°ë„: {scenario['severity']}")
                lines.append(f"      ì„¤ëª…: {scenario['description']}")
                if scenario['sar_required']:
                    lines.append(f"      ğŸš¨ SAR ì œì¶œ í•„ìš”")
        
        if anomalies:
            lines.append(f"\nğŸ“ˆ **ì´ìƒ íƒì§€ëœ ê·¸ë˜í”„ ë©”íŠ¸ë¦­ ({len(anomalies)}ê°œ)**")
            
            feature_details = {
                'density_high': (
                    f"ë°€ì§‘ë„ ê³¼ë„í•˜ê²Œ ë†’ìŒ: {metrics['Density']:.4f}",
                    "â†’ íŠ¹ì • ì§€ê°‘ë“¤ ê°„ ì§‘ì¤‘ëœ ê±°ë˜, ìˆœí™˜ ê±°ë˜ ì˜ì‹¬"
                ),
                'density_low': (
                    f"ë°€ì§‘ë„ ë§¤ìš° ë‚®ìŒ: {metrics['Density']:.6f}",
                    "â†’ ë¶ˆê·œì¹™í•œ ê±°ë˜ íŒ¨í„´, Pump & Dump ì˜ì‹¬"
                ),
                'reciprocity_low': (
                    f"ì–‘ë°©í–¥ ê±°ë˜ ë¹„ìœ¨ ë‚®ìŒ: {metrics['Reciprocity']:.3f}",
                    "â†’ ì¼ë°©ì  ìê¸ˆ íë¦„, ë ˆì´ì–´ë§ ì˜ì‹¬"
                ),
                'assortativity_neg': (
                    f"ìŒìˆ˜ ì—°ê²°ì„±: {metrics['Assortativity']:.3f}",
                    "â†’ ë¹„ì •ìƒì  ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°, ë¯¹ì„œ ì‚¬ìš© ì˜ì‹¬"
                ),
                'clustering_low': (
                    f"ë‚®ì€ í´ëŸ¬ìŠ¤í„°ë§: {metrics['Clustering_Coefficient']:.3f}",
                    "â†’ ë¶„ì‚°ëœ ê±°ë˜ íŒ¨í„´, ìŠ¤ë¨¸í•‘ ê°€ëŠ¥ì„±"
                ),
                'diameter_high': (
                    f"ë„¤íŠ¸ì›Œí¬ ì§ê²½ í¼: {metrics['Effective_Diameter']:.1f}",
                    "â†’ ë„“ê²Œ í™•ì‚°ëœ ê±°ë˜, ë³µì¡í•œ ìê¸ˆ ì´ë™"
                ),
                'nodes_high': (
                    f"ë¹„ì •ìƒì ìœ¼ë¡œ ë§ì€ ì§€ê°‘: {metrics['Num_nodes']:,}ê°œ",
                    "â†’ ëŒ€ê·œëª¨ ë„¤íŠ¸ì›Œí¬, ì¡°ì§ì  í™œë™ ì˜ì‹¬"
                ),
                'edges_ratio_high': (
                    f"ë†’ì€ ê±°ë˜ ë¹ˆë„: í‰ê·  {metrics['Num_edges']/metrics['Num_nodes']:.1f}íšŒ/ì§€ê°‘",
                    "â†’ ê³¼ë„í•œ ê±°ë˜ í™œë™"
                )
            }
            
            for anomaly in anomalies:
                if anomaly in feature_details:
                    detail, interpretation = feature_details[anomaly]
                    lines.append(f"\n  â€¢ {detail}")
                    lines.append(f"    {interpretation}")
        
        lines.append(f"\nğŸ“‹ **ìƒì„¸ ê·¸ë˜í”„ ë©”íŠ¸ë¦­**")
        lines.append(f"  â€¢ ë…¸ë“œ ìˆ˜ (ì§€ê°‘): {metrics['Num_nodes']:,}ê°œ")
        lines.append(f"  â€¢ ì—£ì§€ ìˆ˜ (ê±°ë˜): {metrics['Num_edges']:,}ê°œ")
        lines.append(f"  â€¢ ë°€ì§‘ë„ (Density): {metrics['Density']:.6f}")
        lines.append(f"  â€¢ ì—°ê²°ì„± (Assortativity): {metrics['Assortativity']:.4f}")
        lines.append(f"  â€¢ ì–‘ë°©í–¥ì„± (Reciprocity): {metrics['Reciprocity']:.4f}")
        lines.append(f"  â€¢ ë„¤íŠ¸ì›Œí¬ ì§ê²½ (Effective Diameter): {metrics['Effective_Diameter']:.2f}")
        lines.append(f"  â€¢ í´ëŸ¬ìŠ¤í„°ë§ ê³„ìˆ˜: {metrics['Clustering_Coefficient']:.4f}")
        
        return "\n".join(lines)
    
    def _generate_recommendations(self, risk_score, scenarios):
        """ê¶Œì¥ ì¡°ì¹˜"""
        recommendations = []
        
        if risk_score >= 75:
            recommendations.append("ğŸš¨ ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”")
            recommendations.append("  1. í•´ë‹¹ í† í°ì˜ ëª¨ë“  ê±°ë˜ ì¦‰ì‹œ ëª¨ë‹ˆí„°ë§ ê°•í™”")
            recommendations.append("  2. ì£¼ìš” ì—°ê²° ì§€ê°‘ ì‹ë³„ ë° ì¶”ì ")
            recommendations.append("  3. SAR (Suspicious Activity Report) ì œì¶œ ê²€í† ")
            recommendations.append("  4. í•„ìš”ì‹œ ê±°ë˜ ì¼ì‹œ ì œí•œ ì¡°ì¹˜")
            recommendations.append("  5. ë²• ì§‘í–‰ ê¸°ê´€ ë³´ê³  ê²€í† ")
            
        elif risk_score >= 50:
            recommendations.append("âš ï¸  ë†’ì€ ì£¼ì˜ í•„ìš”")
            recommendations.append("  1. 72ì‹œê°„ ì§‘ì¤‘ ëª¨ë‹ˆí„°ë§")
            recommendations.append("  2. ê±°ë˜ íŒ¨í„´ ìƒì„¸ ë¶„ì„")
            recommendations.append("  3. ê´€ë ¨ ì§€ê°‘ë“¤ì˜ ì¶”ê°€ ì¡°ì‚¬")
            recommendations.append("  4. ë‚´ë¶€ ë³´ê³ ì„œ ì‘ì„±")
            
        elif risk_score >= 30:
            recommendations.append("ğŸ“‹ ì¼ë°˜ ëª¨ë‹ˆí„°ë§")
            recommendations.append("  1. ì •ê¸° ëª¨ë‹ˆí„°ë§ ëŒ€ìƒ ì¶”ê°€")
            recommendations.append("  2. ì›”ê°„ ë¦¬ë·° ì‹œ ì¬í‰ê°€")
            
        else:
            recommendations.append("âœ… ì •ìƒ ë²”ìœ„")
            recommendations.append("  1. í‘œì¤€ ëª¨ë‹ˆí„°ë§ ìœ ì§€")
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ì¶”ê°€ ê¶Œì¥ì‚¬í•­
        for scenario in scenarios:
            if scenario['id'] == 'S1_LAYERING':
                recommendations.append("\nğŸ’¡ ë ˆì´ì–´ë§ ëŒ€ì‘:")
                recommendations.append("  â€¢ ìê¸ˆ íë¦„ ê²½ë¡œ ì™„ì „ ì¶”ì ")
                recommendations.append("  â€¢ ìµœì¢… ëª©ì ì§€ ì§€ê°‘ ì‹ë³„")
                
            elif scenario['id'] == 'S2_SMURFING':
                recommendations.append("\nğŸ’¡ ìŠ¤ë¨¸í•‘ ëŒ€ì‘:")
                recommendations.append("  â€¢ ì†Œì•¡ ê±°ë˜ íŒ¨í„´ ë¶„ì„")
                recommendations.append("  â€¢ ì‹œê°„ëŒ€ë³„ ê±°ë˜ ì§‘ì¤‘ë„ í™•ì¸")
                
            elif scenario['id'] == 'S4_MIXER_PATTERN':
                recommendations.append("\nğŸ’¡ ë¯¹ì„œ ì‚¬ìš© ëŒ€ì‘:")
                recommendations.append("  â€¢ ì•Œë ¤ì§„ ë¯¹ì„œ ì„œë¹„ìŠ¤ì™€ ë¹„êµ")
                recommendations.append("  â€¢ ì…ì¶œê¸ˆ íŒ¨í„´ ìƒì„¸ ë¶„ì„")
        
        return "\n".join(recommendations)
    
    def _generate_reason_code(self, scenarios, anomalies):
        """ì‹œìŠ¤í…œ ì—°ë™ìš© ì´ìœ  ì½”ë“œ"""
        if not scenarios:
            return "R000_NORMAL"
        
        # ì£¼ ì‹œë‚˜ë¦¬ì˜¤
        primary = scenarios[0]['id']
        
        # ì´ìƒ í”¼ì²˜ ì½”ë“œ
        feature_codes = {
            'density_high': 'DH',
            'density_low': 'DL',
            'reciprocity_low': 'RL',
            'assortativity_neg': 'AN',
            'clustering_low': 'CL',
            'diameter_high': 'DH',
            'nodes_high': 'NH',
            'edges_ratio_high': 'EH'
        }
        
        feature_str = ''.join([feature_codes.get(a, '') for a in anomalies[:3]])
        
        return f"{primary}_{feature_str}"


def _load_transactions(chain, contract, transactions_dir):
    tx_path = Path(transactions_dir) / chain / f"{contract}.csv"
    if not tx_path.exists():
        raise FileNotFoundError(f"Transaction file not found: {tx_path}")

    df = pd.read_csv(tx_path)
    missing_columns = [col for col in ["from", "to"] if col not in df.columns]
    if missing_columns:
        raise ValueError(f"Missing required columns {missing_columns} in {tx_path}")

    return df


def _build_graph_from_transactions(df):
    G = nx.DiGraph()
    for _, row in df.iterrows():
        src = row["from"]
        dst = row["to"]
        if pd.isna(src) or pd.isna(dst):
            continue
        G.add_edge(src, dst)
    return G


def _safe_value(value, default=0.0):
    if value is None:
        return default
    if isinstance(value, float) and np.isnan(value):
        return default
    return float(value)


def compute_metrics_from_transactions(chain, contract, transactions_dir="data/transactions"):
    df = _load_transactions(chain, contract, transactions_dir)
    G = _build_graph_from_transactions(df)

    num_nodes = G.number_of_nodes()
    num_edges = G.number_of_edges()

    if num_nodes == 0:
        return {
            "Num_nodes": 0,
            "Num_edges": 0,
            "Density": 0.0,
            "Assortativity": 0.0,
            "Reciprocity": 0.0,
            "Effective_Diameter": 0.0,
            "Clustering_Coefficient": 0.0,
        }

    density = nx.density(G)
    try:
        assortativity = nx.degree_assortativity_coefficient(G)
    except Exception:
        assortativity = 0.0

    reciprocity = _safe_value(nx.overall_reciprocity(G))

    undirected_graph = G.to_undirected()
    if undirected_graph.number_of_edges() == 0 or undirected_graph.number_of_nodes() <= 1:
        effective_diameter = 0.0
        clustering_coefficient = 0.0
    else:
        largest_component_nodes = max(nx.connected_components(undirected_graph), key=len)
        largest_component = undirected_graph.subgraph(largest_component_nodes).copy()

        try:
            effective_diameter = float(nx_approx.diameter(largest_component))
        except Exception:
            effective_diameter = float(len(largest_component_nodes) - 1)

        try:
            clustering_coefficient = nx.average_clustering(largest_component)
        except Exception:
            clustering_coefficient = 0.0

    return {
        "Num_nodes": num_nodes,
        "Num_edges": num_edges,
        "Density": _safe_value(density, 0.0),
        "Assortativity": _safe_value(assortativity, 0.0),
        "Reciprocity": _safe_value(reciprocity, 0.0),
        "Effective_Diameter": _safe_value(effective_diameter, 0.0),
        "Clustering_Coefficient": _safe_value(clustering_coefficient, 0.0),
    }


def run_real_analysis(chain, contract, transactions_dir="data/transactions"):
    detector = ExplainableAMLDetector()
    try:
        metrics = compute_metrics_from_transactions(chain, contract, transactions_dir)
    except FileNotFoundError as exc:
        print(f"âŒ {exc}")
        return
    except ValueError as exc:
        print(f"âŒ ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜: {exc}")
        return

    result = detector.analyze_token(contract, metrics)

    print("\n" + "=" * 70)
    print(f"ğŸ” ì‹¤ë°ì´í„° AML ë¶„ì„ - {chain}/{contract}")
    print("=" * 70)
    print(result["explanation"])
    print(f"\nì´ìœ  ì½”ë“œ: {result['reason_code']}")
    print(f"SAR ì œì¶œ í•„ìš”: {'ì˜ˆ ğŸš¨' if result['requires_sar'] else 'ì•„ë‹ˆì˜¤'}")
    print(f"\n{result['recommendations']}")


def build_arg_parser():
    parser = argparse.ArgumentParser(
        description="Explainable AML detector demo. "
        "Run without arguments for canned examples or provide --chain and --contract to analyze real data."
    )
    parser.add_argument(
        "--chain",
        choices=["bsc", "ethereum", "polygon"],
        help="Chain name corresponding to subdirectory under data/transactions/",
    )
    parser.add_argument(
        "--contract",
        help="Contract address (CSV filename without extension) to analyze.",
    )
    parser.add_argument(
        "--transactions-dir",
        default="data/transactions",
        help="Root directory containing chain subdirectories with transaction CSV files.",
    )
    return parser


def run_demo():
    """ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ ë°ëª¨"""
    
    print("\n" + "=" * 70)
    print("ğŸ¦ ë¸”ë¡ì²´ì¸ AML ì‹œìŠ¤í…œ - ì‹¤ì œ ê²°ê³¼ ì˜ˆì‹œ")
    print("=" * 70)
    
    detector = ExplainableAMLDetector()
    
    # ì˜ˆì‹œ 1: ì •ìƒ í† í°
    print("\n\n[ì˜ˆì‹œ 1] ì •ìƒ í† í°")
    print("-" * 70)
    normal_token = {
        'Num_nodes': 234,
        'Num_edges': 567,
        'Density': 0.15,
        'Assortativity': 0.12,
        'Reciprocity': 0.45,
        'Effective_Diameter': 5.2,
        'Clustering_Coefficient': 0.32
    }
    
    result1 = detector.analyze_token('0x1234...Normal', normal_token)
    print(result1['explanation'])
    print(f"\nì´ìœ  ì½”ë“œ: {result1['reason_code']}")
    print(f"\n{result1['recommendations']}")
    
    # ì˜ˆì‹œ 2: ë ˆì´ì–´ë§ ì˜ì‹¬
    print("\n\n" + "=" * 70)
    print("[ì˜ˆì‹œ 2] ë ˆì´ì–´ë§ ì˜ì‹¬ í† í°")
    print("-" * 70)
    layering_token = {
        'Num_nodes': 8543,
        'Num_edges': 34219,
        'Density': 0.87,
        'Assortativity': -0.15,
        'Reciprocity': 0.08,
        'Effective_Diameter': 18.7,
        'Clustering_Coefficient': 0.04
    }
    
    result2 = detector.analyze_token('0xABCD...Suspicious', layering_token)
    print(result2['explanation'])
    print(f"\nì´ìœ  ì½”ë“œ: {result2['reason_code']}")
    print(f"SAR ì œì¶œ í•„ìš”: {'ì˜ˆ ğŸš¨' if result2['requires_sar'] else 'ì•„ë‹ˆì˜¤'}")
    print(f"\n{result2['recommendations']}")
    
    # ì˜ˆì‹œ 3: ìŠ¤ë¨¸í•‘ ì˜ì‹¬
    print("\n\n" + "=" * 70)
    print("[ì˜ˆì‹œ 3] ìŠ¤ë¨¸í•‘ ì˜ì‹¬ í† í°")
    print("-" * 70)
    smurfing_token = {
        'Num_nodes': 15234,
        'Num_edges': 52891,
        'Density': 0.03,
        'Assortativity': 0.05,
        'Reciprocity': 0.07,
        'Effective_Diameter': 12.3,
        'Clustering_Coefficient': 0.18
    }
    
    result3 = detector.analyze_token('0xDEF0...Smurfing', smurfing_token)
    print(result3['explanation'])
    print(f"\nì´ìœ  ì½”ë“œ: {result3['reason_code']}")
    print(f"SAR ì œì¶œ í•„ìš”: {'ì˜ˆ ğŸš¨' if result3['requires_sar'] else 'ì•„ë‹ˆì˜¤'}")
    print(f"\n{result3['recommendations']}")
    
    # ìš”ì•½ í…Œì´ë¸”
    print("\n\n" + "=" * 70)
    print("ğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½")
    print("=" * 70)
    print(f"{'í† í°':20} {'ë¦¬ìŠ¤í¬ ì ìˆ˜':12} {'ë ˆë²¨':10} {'ì£¼ìš” ì‹œë‚˜ë¦¬ì˜¤':25} {'SAR':5}")
    print("-" * 70)
    
    for result in [result1, result2, result3]:
        token = result['token_address'][:20].ljust(20)
        score = f"{result['risk_score']:.1f}/100".ljust(12)
        level = result['risk_level'].ljust(10)
        scenario = (result['matched_scenarios'][0]['name'][:25] if result['matched_scenarios'] else '-').ljust(25)
        sar = 'í•„ìš”' if result['requires_sar'] else '-'
        
        print(f"{token} {score} {level} {scenario} {sar}")


if __name__ == "__main__":
    parser = build_arg_parser()
    args = parser.parse_args()

    if args.chain and args.contract:
        run_real_analysis(args.chain, args.contract, args.transactions_dir)
    elif args.chain or args.contract:
        parser.error("Both --chain and --contract must be provided to analyze real data.")
    else:
        run_demo()

