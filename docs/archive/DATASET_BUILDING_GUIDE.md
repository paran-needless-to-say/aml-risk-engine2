# í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ êµ¬ì¶• ê°€ì´ë“œ

## ğŸ“Š í˜„ì¬ ë°ì´í„° í˜„í™©

### ë ˆê±°ì‹œ ë°ì´í„°

1. **ê±°ë˜ ë°ì´í„°**: `legacy/data/transactions/{chain}/{address}.csv`

   - ê±°ë˜ ë‚´ì—­ì€ ìˆìŒ
   - ë¼ë²¨ë§ì€ ì—†ìŒ

2. **Features ë°ì´í„°**: `legacy/data/features/{chain}_basic_metrics_processed.csv`

   - ê·¸ë˜í”„ ë©”íŠ¸ë¦­ í¬í•¨
   - **ë¼ë²¨ ì»¬ëŸ¼ ìˆìŒ** (`label`: 0=normal, 1=fraud)
   - í•˜ì§€ë§Œ ê±°ë˜ ë ˆë²¨ ë¼ë²¨ì€ ì•„ë‹˜

3. **Demo ë°ì´í„°**: `demo/transactions/*.json`
   - ì‹œë‚˜ë¦¬ì˜¤ë³„ë¡œ ë¶„ë¥˜ë¨
   - `high_risk`, `medium_risk`, `low_risk`ë¡œ êµ¬ë¶„
   - ë¼ë²¨ë§ ê°€ëŠ¥

---

## ğŸ¯ ë°ì´í„°ì…‹ êµ¬ì¶• ë°©ë²• 4ê°€ì§€

### ë°©ë²• 0: Etherscan APIë¡œ ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ (ê°€ì¥ ì •í™•) â­ **ì¶”ì²œ**

**ì¥ì **:

- ì‹¤ì œ ë¸”ë¡ì²´ì¸ ë°ì´í„°
- ëŒ€ëŸ‰ ë°ì´í„° ìˆ˜ì§‘ ê°€ëŠ¥
- ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥
- ì •í™•í•œ ë¼ë²¨ë§ ê°€ëŠ¥ (OFAC, ë¯¹ì„œ ë¦¬ìŠ¤íŠ¸ í™œìš©)

**êµ¬í˜„**:

```python
from core.scoring.real_dataset_builder import RealDatasetBuilder
import os

# API í‚¤ ì„¤ì •
api_key = os.getenv("ETHERSCAN_API_KEY")

# ë°ì´í„°ì…‹ êµ¬ì¶•ê¸° ìƒì„±
builder = RealDatasetBuilder(api_key=api_key, chain="ethereum")

# ê³ ìœ„í—˜ ì£¼ì†Œ ë¦¬ìŠ¤íŠ¸ (OFAC, ë¯¹ì„œ)
from core.data.lists import ListLoader
list_loader = ListLoader()
high_risk_addresses = list(list_loader.get_sdn_list())[:20] + \
                      list(list_loader.get_mixer_list())[:20]

# ë°ì´í„°ì…‹ êµ¬ì¶•
dataset = builder.build_from_high_risk_addresses(
    addresses=high_risk_addresses,
    max_transactions_per_address=50,
    output_path="data/dataset/real_etherscan.json"
)
```

**ì‚¬ìš©ë²•**:

```bash
# 1. API í‚¤ ë°œê¸‰: https://etherscan.io/apis
# 2. í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export ETHERSCAN_API_KEY="your_api_key_here"

# 3. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
python scripts/collect_real_data.py
```

**ë¼ë²¨ë§**:

- OFAC ì œì¬ ì£¼ì†Œ ê±°ë˜ â†’ fraud (85ì )
- ë¯¹ì„œ ì£¼ì†Œ ê±°ë˜ â†’ fraud (85ì )
- ì •ìƒ ì£¼ì†Œ ê±°ë˜ â†’ normal (15ì )

---

### ë°©ë²• 1: Demo ì‹œë‚˜ë¦¬ì˜¤ í™œìš© (ê°€ì¥ ì‰¬ì›€) âœ…

**ì¥ì **:

- ì´ë¯¸ ì‹œë‚˜ë¦¬ì˜¤ë³„ë¡œ ë¶„ë¥˜ë˜ì–´ ìˆìŒ
- ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥
- ë‹¤ì–‘í•œ ë¦¬ìŠ¤í¬ ë ˆë²¨ í¬í•¨

**êµ¬í˜„**:

```python
from core.scoring.dataset_builder import DatasetBuilder

builder = DatasetBuilder()
dataset = builder.build_from_demo_scenarios(
    demo_dir="demo/transactions",
    output_path="data/dataset/demo_labeled.json"
)
```

**ì‹œë‚˜ë¦¬ì˜¤ ë§¤í•‘**:

- `high_risk_*` â†’ fraud (85ì )
- `medium_risk_*` â†’ suspicious (60ì )
- `low_risk_*` â†’ normal (15ì )

---

### ë°©ë²• 2: ë ˆê±°ì‹œ Features ë°ì´í„° í™œìš©

**ì¥ì **:

- ëŒ€ëŸ‰ ë°ì´í„° ê°€ëŠ¥
- ì‹¤ì œ ë¸”ë¡ì²´ì¸ ë°ì´í„°

**ë‹¨ì **:

- ì£¼ì†Œ ë ˆë²¨ ë¼ë²¨ë§Œ ìˆìŒ (ê±°ë˜ ë ˆë²¨ ì•„ë‹˜)
- USD ë³€í™˜ í•„ìš”

**êµ¬í˜„**:

```python
builder = DatasetBuilder()
dataset = builder.build_from_legacy_features(
    features_path="legacy/data/features/ethereum_basic_metrics_processed.csv",
    transactions_dir="legacy/data/transactions",
    output_path="data/dataset/legacy_labeled.json"
)
```

**ë¼ë²¨ ë§¤í•‘**:

- `label=1` â†’ fraud (85ì )
- `label=0` â†’ normal (15ì )

---

### ë°©ë²• 3: ê·œì¹™ ê¸°ë°˜ ìë™ ë¼ë²¨ë§ (ì´ˆê¸° ë°ì´í„°ì…‹)

**ì¥ì **:

- ëŒ€ëŸ‰ ë°ì´í„° ìë™ ìƒì„± ê°€ëŠ¥
- ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥

**ë‹¨ì **:

- í˜„ì¬ ë£°ì˜ í•œê³„ë¥¼ ê·¸ëŒ€ë¡œ ë°˜ì˜
- ì‹¤ì œ ë¼ë²¨ê³¼ ì°¨ì´ ìˆì„ ìˆ˜ ìˆìŒ

**êµ¬í˜„**:

```python
# ê±°ë˜ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„
transactions = [...]  # ë°±ì—”ë“œì—ì„œ ë°›ì€ ê±°ë˜ ë¦¬ìŠ¤íŠ¸

builder = DatasetBuilder()
dataset = builder.build_from_rule_based_labeling(
    transactions,
    output_path="data/dataset/rule_based_labeled.json"
)
```

**ë¼ë²¨ ê²°ì •**:

- ê·œì¹™ ê¸°ë°˜ ì ìˆ˜ >= 60 â†’ fraud
- ê·œì¹™ ê¸°ë°˜ ì ìˆ˜ >= 30 â†’ suspicious
- ê·œì¹™ ê¸°ë°˜ ì ìˆ˜ < 30 â†’ normal

---

## ğŸ“‹ ë°ì´í„°ì…‹ êµ¬ì¡°

### í•™ìŠµ ë°ì´í„° í˜•ì‹

```json
{
  "rule_results": [
    {
      "rule_id": "C-001",
      "score": 30,
      "axis": "C",
      "severity": "HIGH",
      "name": "Sanction Direct Touch"
    },
    {
      "rule_id": "E-101",
      "score": 25,
      "axis": "E",
      "severity": "HIGH",
      "name": "Mixer Direct Exposure"
    }
  ],
  "actual_risk_score": 85.0,
  "tx_context": {
    "amount_usd": 5000.0,
    "is_sanctioned": true,
    "is_mixer": true,
    "chain": "ethereum"
  },
  "ground_truth_label": "fraud",
  "tx_hash": "0x...",
  "chain": "ethereum"
}
```

---

## ğŸ”„ ë°ì´í„°ì…‹ ë¶„í• 

### í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• 

```python
from core.scoring.dataset_builder import DatasetBuilder

builder = DatasetBuilder()
dataset = builder.build_from_demo_scenarios(...)

# ë¶„í•  (ë¼ë²¨ë³„ ë¹„ìœ¨ ìœ ì§€)
train, val, test = builder.split_dataset(
    dataset,
    train_ratio=0.7,
    val_ratio=0.15,
    test_ratio=0.15,
    stratify=True  # ë¼ë²¨ë³„ ë¹„ìœ¨ ìœ ì§€
)

# ì €ì¥
with open("data/dataset/train.json", 'w') as f:
    json.dump(train, f, indent=2)

with open("data/dataset/val.json", 'w') as f:
    json.dump(val, f, indent=2)

with open("data/dataset/test.json", 'w') as f:
    json.dump(test, f, indent=2)
```

---

## ğŸ‘¨â€ğŸ’¼ ì „ë¬¸ê°€ ë¼ë²¨ë§ (ì„ íƒì )

### ë¼ë²¨ë§ í…œí”Œë¦¿ ìƒì„±

```python
from core.scoring.dataset_builder import ExpertLabelingTool

tool = ExpertLabelingTool("data/dataset/rule_based_labeled.json")
tool.create_labeling_template("data/dataset/labeling_template.json")
```

### ë¼ë²¨ë§ í…œí”Œë¦¿ í˜•ì‹

```json
[
  {
    "id": 0,
    "tx_hash": "0x...",
    "rule_results": [...],
    "rule_based_score": 55.0,
    "expert_label": null,  // ì „ë¬¸ê°€ê°€ ì±„ì›€: "fraud" | "suspicious" | "normal"
    "expert_score": null,  // ì „ë¬¸ê°€ê°€ ì±„ì›€: 0~100
    "notes": ""  // ë©”ëª¨
  },
  ...
]
```

### ë¼ë²¨ë§ëœ ë°ì´í„° ë¡œë“œ

```python
labeled_data = tool.load_labeled_data("data/dataset/labeling_template.json")
```

---

## ğŸš€ ì‹¤ì œ êµ¬ì¶• ë‹¨ê³„

### 1ë‹¨ê³„: Demo ë°ì´í„°ë¡œ ì´ˆê¸° ë°ì´í„°ì…‹ êµ¬ì¶• (ì¦‰ì‹œ)

```python
# scripts/build_initial_dataset.py
from core.scoring.dataset_builder import DatasetBuilder

builder = DatasetBuilder()

# Demo ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ êµ¬ì¶•
dataset = builder.build_from_demo_scenarios(
    demo_dir="demo/transactions",
    output_path="data/dataset/initial.json"
)

# ë¶„í• 
train, val, test = builder.split_dataset(dataset)

print(f"í•™ìŠµ: {len(train)}ê°œ")
print(f"ê²€ì¦: {len(val)}ê°œ")
print(f"í…ŒìŠ¤íŠ¸: {len(test)}ê°œ")
```

**ì˜ˆìƒ ê²°ê³¼**: ì•½ 100~200ê°œ ìƒ˜í”Œ (Demo ë°ì´í„° ê¸°ì¤€)

---

### 2ë‹¨ê³„: ë ˆê±°ì‹œ ë°ì´í„° í™•ì¥ (1-2ì£¼)

```python
# ë ˆê±°ì‹œ features í™œìš©
legacy_dataset = builder.build_from_legacy_features(
    features_path="legacy/data/features/ethereum_basic_metrics_processed.csv",
    transactions_dir="legacy/data/transactions",
    output_path="data/dataset/legacy.json"
)

# Demoì™€ ë³‘í•©
combined_dataset = demo_dataset + legacy_dataset

# ë¶„í• 
train, val, test = builder.split_dataset(combined_dataset)
```

**ì˜ˆìƒ ê²°ê³¼**: ìˆ˜ì²œ~ìˆ˜ë§Œ ê°œ ìƒ˜í”Œ

---

### 3ë‹¨ê³„: ê·œì¹™ ê¸°ë°˜ ìë™ ë¼ë²¨ë§ (ì§€ì†ì )

```python
# ë°±ì—”ë“œì—ì„œ ë°›ì€ ê±°ë˜ ë°ì´í„°ë¡œ ìë™ ë¼ë²¨ë§
new_transactions = [...]  # ë°±ì—”ë“œ APIì—ì„œ ë°›ì€ ê±°ë˜

rule_based_dataset = builder.build_from_rule_based_labeling(
    new_transactions,
    output_path="data/dataset/rule_based_new.json"
)

# ê¸°ì¡´ ë°ì´í„°ì…‹ì— ì¶”ê°€
existing_dataset.extend(rule_based_dataset)
```

**ì¥ì **: ì§€ì†ì ìœ¼ë¡œ ë°ì´í„°ì…‹ í™•ì¥ ê°€ëŠ¥

---

### 4ë‹¨ê³„: ì „ë¬¸ê°€ ë¼ë²¨ë§ (ì„ íƒì , ì¥ê¸°)

```python
# ê³ ìœ„í—˜ ìƒ˜í”Œë§Œ ì „ë¬¸ê°€ ë¼ë²¨ë§
high_risk_samples = [
    d for d in dataset
    if sum(r.get("score", 0) for r in d["rule_results"]) >= 50
]

tool = ExpertLabelingTool("data/dataset/high_risk.json")
tool.create_labeling_template("data/dataset/expert_labeling.json")

# ì „ë¬¸ê°€ê°€ ë¼ë²¨ë§ í›„
labeled_data = tool.load_labeled_data("data/dataset/expert_labeling.json")
```

---

## ğŸ“Š ë°ì´í„°ì…‹ í†µê³„

### ë¼ë²¨ ë¶„í¬ í™•ì¸

```python
def analyze_dataset(dataset):
    """ë°ì´í„°ì…‹ í†µê³„ ë¶„ì„"""
    labels = [d.get("ground_truth_label") for d in dataset]
    label_counts = {}
    for label in labels:
        label_counts[label] = label_counts.get(label, 0) + 1

    print("ë¼ë²¨ ë¶„í¬:")
    for label, count in label_counts.items():
        print(f"  {label}: {count}ê°œ ({count/len(dataset)*100:.1f}%)")

    # ì ìˆ˜ ë¶„í¬
    scores = [d.get("actual_risk_score", 0) for d in dataset]
    print(f"\nì ìˆ˜ í†µê³„:")
    print(f"  í‰ê· : {sum(scores)/len(scores):.1f}")
    print(f"  ìµœì†Œ: {min(scores):.1f}")
    print(f"  ìµœëŒ€: {max(scores):.1f}")

analyze_dataset(train)
```

---

## ğŸ¯ ê¶Œì¥ ì ‘ê·¼ë²•

### ì¦‰ì‹œ ì‹œì‘ (1ì¼)

1. **Demo ë°ì´í„°ë¡œ ì´ˆê¸° ë°ì´í„°ì…‹ êµ¬ì¶•**

   ```bash
   python scripts/build_initial_dataset.py
   ```

2. **ê·œì¹™ ê¸°ë°˜ ê°€ì¤‘ì¹˜ë¡œ ëª¨ë¸ í•™ìŠµ**

   ```python
   from core.scoring.ai_weight_learner import RuleWeightLearner

   learner = RuleWeightLearner(use_ai=False)  # ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì‹œì‘
   # ì´ë¯¸ ì‘ë™ ì¤‘!
   ```

---

### ë‹¨ê¸° (1-2ì£¼)

1. **ë ˆê±°ì‹œ ë°ì´í„° í™•ì¥**

   - Features ë°ì´í„° í™œìš©
   - ê±°ë˜ ë°ì´í„°ì™€ ë§¤ì¹­

2. **ë°ì´í„°ì…‹ í’ˆì§ˆ ê°œì„ **
   - ì´ìƒì¹˜ ì œê±°
   - ë¼ë²¨ ë¶„í¬ ê· í˜• ì¡°ì •

---

### ì¤‘ê¸° (1ê°œì›”)

1. **ì „ë¬¸ê°€ ë¼ë²¨ë§**

   - ê³ ìœ„í—˜ ìƒ˜í”Œ ìš°ì„  ë¼ë²¨ë§
   - ì ì§„ì  í™•ì¥

2. **AI ëª¨ë¸ í•™ìŠµ**
   ```python
   learner = RuleWeightLearner(use_ai=True)
   learner.train(training_data)
   ```

---

## ğŸ’¡ ë¼ë²¨ì´ ì—†ì„ ë•Œ ëŒ€ì•ˆ

### 1. ê·œì¹™ ê¸°ë°˜ ìë™ ë¼ë²¨ë§

í˜„ì¬ ë£°ë¡œ ìŠ¤ì½”ì–´ë§í•œ ê²°ê³¼ë¥¼ ë¼ë²¨ë¡œ ì‚¬ìš©:

- ì ìˆ˜ >= 60 â†’ fraud
- ì ìˆ˜ >= 30 â†’ suspicious
- ì ìˆ˜ < 30 â†’ normal

**ì¥ì **: ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥, ëŒ€ëŸ‰ ìƒì„± ê°€ëŠ¥

**ë‹¨ì **: í˜„ì¬ ë£°ì˜ í•œê³„ ë°˜ì˜

---

### 2. ì™¸ë¶€ ë°ì´í„° ì†ŒìŠ¤ í™œìš©

- **OFAC SDN ë¦¬ìŠ¤íŠ¸**: ì œì¬ ì£¼ì†Œ = fraud
- **ë¯¹ì„œ ë¦¬ìŠ¤íŠ¸**: ë¯¹ì„œ ê±°ë˜ = suspicious
- **CEX ì£¼ì†Œ**: CEX ê±°ë˜ = normal (ì¼ë°˜ì ìœ¼ë¡œ)

---

### 3. ë°˜ì§€ë„ í•™ìŠµ (Semi-supervised Learning)

- ì†ŒëŸ‰ì˜ ë¼ë²¨ë§ëœ ë°ì´í„°ë¡œ ì‹œì‘
- ëŒ€ëŸ‰ì˜ ë¼ë²¨ë§ë˜ì§€ ì•Šì€ ë°ì´í„° í™œìš©
- ì ì§„ì  ë¼ë²¨ë§

---

## ğŸ“ ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ

### ì „ì²´ íŒŒì´í”„ë¼ì¸

```python
# scripts/build_training_dataset.py
from core.scoring.dataset_builder import DatasetBuilder

builder = DatasetBuilder()

# 1. Demo ë°ì´í„°
demo_dataset = builder.build_from_demo_scenarios(
    demo_dir="demo/transactions",
    output_path="data/dataset/demo.json"
)

# 2. ë ˆê±°ì‹œ ë°ì´í„° (ì„ íƒì )
# legacy_dataset = builder.build_from_legacy_features(...)

# 3. ê·œì¹™ ê¸°ë°˜ ìë™ ë¼ë²¨ë§ (ì„ íƒì )
# rule_based = builder.build_from_rule_based_labeling(...)

# 4. ë³‘í•©
all_dataset = demo_dataset  # + legacy_dataset + rule_based

# 5. ë¶„í• 
train, val, test = builder.split_dataset(
    all_dataset,
    train_ratio=0.7,
    val_ratio=0.15,
    test_ratio=0.15,
    stratify=True
)

# 6. ì €ì¥
import json
with open("data/dataset/train.json", 'w') as f:
    json.dump(train, f, indent=2)
with open("data/dataset/val.json", 'w') as f:
    json.dump(val, f, indent=2)
with open("data/dataset/test.json", 'w') as f:
    json.dump(test, f, indent=2)

print(f"ë°ì´í„°ì…‹ êµ¬ì¶• ì™„ë£Œ!")
print(f"  í•™ìŠµ: {len(train)}ê°œ")
print(f"  ê²€ì¦: {len(val)}ê°œ")
print(f"  í…ŒìŠ¤íŠ¸: {len(test)}ê°œ")
```

---

## ğŸ”— ì°¸ê³ 

- `core/scoring/dataset_builder.py`: ë°ì´í„°ì…‹ êµ¬ì¶• ëª¨ë“ˆ
- `core/scoring/ai_weight_learner.py`: AI ê°€ì¤‘ì¹˜ í•™ìŠµ ëª¨ë“ˆ
- `demo/transactions/`: Demo ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„°
- `legacy/data/features/`: ë ˆê±°ì‹œ features ë°ì´í„°
