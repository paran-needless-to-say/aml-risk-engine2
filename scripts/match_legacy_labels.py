#!/usr/bin/env python3
"""
ë ˆê±°ì‹œ features ë¼ë²¨ì„ ê±°ë˜ ë°ì´í„°ì™€ ë§¤ì¹­ (í…ŒìŠ¤íŠ¸ìš©)

ì‚¬ìš©ë²•:
    # ì‘ì€ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ (10%, ì£¼ì†Œë‹¹ ìµœëŒ€ 50ê±´)
    python scripts/match_legacy_labels.py
    
    # ë” ì‘ì€ ìƒ˜í”Œ
    python scripts/match_legacy_labels.py --sample-ratio 0.05 --max-txs-per-contract 20
"""
import sys
import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.scoring.dataset_builder import DatasetBuilder
from tqdm import tqdm


def match_legacy_labels(
    features_path: str,
    transactions_dir: str,
    output_path: str,
    max_transactions_per_contract: int = None,
    sample_ratio: float = 1.0
) -> List[Dict[str, Any]]:
    """
    ë ˆê±°ì‹œ features ë¼ë²¨ì„ ê±°ë˜ ë°ì´í„°ì™€ ë§¤ì¹­
    
    Args:
        features_path: features CSV íŒŒì¼ ê²½ë¡œ
        transactions_dir: ê±°ë˜ ë°ì´í„° ë””ë ‰í† ë¦¬
        output_path: ì¶œë ¥ JSON íŒŒì¼ ê²½ë¡œ
        max_transactions_per_contract: ì£¼ì†Œë‹¹ ìµœëŒ€ ê±°ë˜ ìˆ˜ (Noneì´ë©´ ëª¨ë‘)
        sample_ratio: ìƒ˜í”Œë§ ë¹„ìœ¨ (1.0 = 100%, 0.1 = 10%)
    
    Returns:
        í•™ìŠµ ë°ì´í„°ì…‹ ë¦¬ìŠ¤íŠ¸
    """
    print("=" * 60)
    print("ë ˆê±°ì‹œ ë¼ë²¨ ë§¤ì¹­ ì‹œì‘ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)")
    print("=" * 60)
    
    # Features ë¡œë“œ
    print(f"\nğŸ“‚ Features íŒŒì¼ ë¡œë“œ: {features_path}")
    df = pd.read_csv(features_path)
    print(f"   ì´ {len(df)}ê°œ ì£¼ì†Œ")
    
    # ì´ë”ë¦¬ì›€ë§Œ í•„í„°ë§
    df_eth = df[df['Chain'].str.lower() == 'ethereum'].copy()
    print(f"   ì´ë”ë¦¬ì›€: {len(df_eth)}ê°œ ì£¼ì†Œ")
    
    # ìƒ˜í”Œë§ (í…ŒìŠ¤íŠ¸ìš©)
    if sample_ratio < 1.0:
        df_eth = df_eth.sample(frac=sample_ratio, random_state=42)
        print(f"   ìƒ˜í”Œë§: {len(df_eth)}ê°œ ì£¼ì†Œ ({sample_ratio*100:.0f}%)")
    
    # ë¼ë²¨ ë¶„í¬ í™•ì¸
    label_counts = df_eth['label'].value_counts()
    print(f"\nğŸ“Š ë¼ë²¨ ë¶„í¬:")
    print(f"   Normal (0): {label_counts.get(0, 0)}ê°œ")
    print(f"   Fraud (1): {label_counts.get(1, 0)}ê°œ")
    
    # ë°ì´í„°ì…‹ êµ¬ì¶•ê¸° ìƒì„±
    builder = DatasetBuilder()
    
    dataset = []
    transactions_dir_path = Path(transactions_dir)
    processed_count = 0
    skipped_count = 0
    
    print(f"\nğŸ”„ ê±°ë˜ ë°ì´í„° ë§¤ì¹­ ì¤‘...")
    print(f"   ì£¼ì†Œë‹¹ ìµœëŒ€ ê±°ë˜ ìˆ˜: {max_transactions_per_contract or 'ì œí•œ ì—†ìŒ'}")
    
    # ì§„í–‰ ìƒí™© í‘œì‹œ
    for idx, row in tqdm(df_eth.iterrows(), total=len(df_eth), desc="ì£¼ì†Œ ì²˜ë¦¬"):
        chain = row['Chain'].lower()
        contract = row['Contract']
        label = int(row.get('label', 0))
        
        # ê±°ë˜ ë°ì´í„° íŒŒì¼ ê²½ë¡œ
        tx_file = transactions_dir_path / chain / f"{contract}.csv"
        
        if not tx_file.exists():
            skipped_count += 1
            continue
        
        try:
            # ê±°ë˜ ë°ì´í„° ì½ê¸°
            df_tx = pd.read_csv(tx_file)
            
            # ìµœëŒ€ ê±°ë˜ ìˆ˜ ì œí•œ
            if max_transactions_per_contract and len(df_tx) > max_transactions_per_contract:
                df_tx = df_tx.sample(n=max_transactions_per_contract, random_state=42)
            
            # ê±°ë˜ ë°ì´í„° ë³€í™˜
            transactions = []
            for _, tx_row in df_tx.iterrows():
                tx = {
                    "tx_hash": str(tx_row.get("transaction_hash", "")),
                    "from": str(tx_row.get("from", "")),
                    "to": str(tx_row.get("to", "")),
                    "timestamp": int(tx_row.get("timestamp", 0)) if pd.notna(tx_row.get("timestamp")) else 0,
                    "usd_value": 0.0,  # USD ë³€í™˜ í•„ìš” (ì„ íƒì )
                    "chain": chain,
                    "asset_contract": contract,
                    "block_height": int(tx_row.get("block_number", 0)) if pd.notna(tx_row.get("block_number")) else 0,
                }
                transactions.append(tx)
            
            if not transactions:
                skipped_count += 1
                continue
            
            # ê° ê±°ë˜ì— ëŒ€í•´ ë£° í‰ê°€ ë° ë°ì´í„°ì…‹ ì¶”ê°€
            for tx in transactions:
                # ë£° í‰ê°€ìš© ë°ì´í„° ë³€í™˜
                tx_for_eval = builder._convert_transaction(tx)
                
                # ë£° í‰ê°€
                rule_results = builder.rule_evaluator.evaluate_single_transaction(tx_for_eval)
                
                # ì‹¤ì œ ë¦¬ìŠ¤í¬ ì ìˆ˜ ê³„ì‚° (ë¼ë²¨ ê¸°ë°˜)
                actual_score = builder._label_to_score(label)
                
                # ì»¨í…ìŠ¤íŠ¸
                tx_context = {
                    "amount_usd": tx.get("usd_value", 0),
                    "is_sanctioned": False,  # featuresì—ëŠ” ì—†ìŒ
                    "is_mixer": False,  # featuresì—ëŠ” ì—†ìŒ
                    "chain": chain,
                }
                
                dataset.append({
                    "rule_results": rule_results,
                    "actual_risk_score": actual_score,
                    "tx_context": tx_context,
                    "ground_truth_label": "fraud" if label == 1 else "normal",
                    "tx_hash": tx.get("tx_hash", ""),
                    "chain": chain,
                    "contract": contract,
                    "data_source": "legacy_features"
                })
            
            processed_count += 1
        
        except Exception as e:
            print(f"\nâš ï¸  ì—ëŸ¬ ({contract}): {e}")
            skipped_count += 1
            continue
    
    # ì €ì¥
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ’¾ ë°ì´í„°ì…‹ ì €ì¥ ì¤‘: {output_path}")
    with open(output_file, 'w') as f:
        json.dump(dataset, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… ë§¤ì¹­ ì™„ë£Œ!")
    print(f"   ì²˜ë¦¬ëœ ì£¼ì†Œ: {processed_count}ê°œ")
    print(f"   ê±´ë„ˆë›´ ì£¼ì†Œ: {skipped_count}ê°œ")
    print(f"   ì´ ìƒ˜í”Œ ìˆ˜: {len(dataset)}ê°œ")
    
    # ìµœì¢… í†µê³„
    if dataset:
        fraud_count = sum(1 for d in dataset if d['ground_truth_label'] == 'fraud')
        normal_count = sum(1 for d in dataset if d['ground_truth_label'] == 'normal')
        
        print(f"\nğŸ“Š ìµœì¢… ë¼ë²¨ ë¶„í¬:")
        print(f"   Fraud: {fraud_count}ê°œ ({fraud_count/len(dataset)*100:.1f}%)")
        print(f"   Normal: {normal_count}ê°œ ({normal_count/len(dataset)*100:.1f}%)")
    
    return dataset


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ë ˆê±°ì‹œ features ë¼ë²¨ì„ ê±°ë˜ ë°ì´í„°ì™€ ë§¤ì¹­")
    parser.add_argument(
        "--features-path",
        default="legacy/data/features/ethereum_basic_metrics_processed.csv",
        help="Features CSV íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--transactions-dir",
        default="legacy/data/transactions",
        help="ê±°ë˜ ë°ì´í„° ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        "--output-path",
        default="data/dataset/legacy_ethereum_test.json",
        help="ì¶œë ¥ JSON íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--max-txs-per-contract",
        type=int,
        default=50,
        help="ì£¼ì†Œë‹¹ ìµœëŒ€ ê±°ë˜ ìˆ˜ (ê¸°ë³¸ê°’: 50, í…ŒìŠ¤íŠ¸ìš©)"
    )
    parser.add_argument(
        "--sample-ratio",
        type=float,
        default=0.1,
        help="ìƒ˜í”Œë§ ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.1 = 10%, í…ŒìŠ¤íŠ¸ìš©)"
    )
    
    args = parser.parse_args()
    
    # ë§¤ì¹­ ì‹¤í–‰
    dataset = match_legacy_labels(
        features_path=args.features_path,
        transactions_dir=args.transactions_dir,
        output_path=args.output_path,
        max_transactions_per_contract=args.max_txs_per_contract,
        sample_ratio=args.sample_ratio
    )
    
    print(f"\nğŸ“ ê²°ê³¼ íŒŒì¼: {args.output_path}")
    print(f"\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print(f"   ì „ì²´ ë°ì´í„° ì²˜ë¦¬: python scripts/match_legacy_labels.py --sample-ratio 1.0 --max-txs-per-contract 100")
    print(f"   ë°ì´í„°ì…‹ ë¶„í• : python scripts/split_dataset.py")


if __name__ == "__main__":
    main()

