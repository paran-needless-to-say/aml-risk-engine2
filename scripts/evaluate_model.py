#!/usr/bin/env python3
"""
ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

Baseline ëª¨ë¸ê³¼ AI ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµ í‰ê°€

ì‚¬ìš©ë²•:
    python scripts/evaluate_model.py
"""
import json
import sys
import pickle
from pathlib import Path
from typing import Dict, List, Any, Tuple
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.scoring.ai_weight_learner import RuleWeightLearner
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score, confusion_matrix,
    classification_report
)


class BaselineModels:
    """Baseline ëª¨ë¸ë“¤"""
    
    @staticmethod
    def simple_sum(rule_results: List[Dict[str, Any]]) -> float:
        """Baseline 1: ë‹¨ìˆœ í•©ì‚° (ê°€ì¥ ê¸°ë³¸)"""
        return min(100.0, sum(r.get("score", 0) for r in rule_results))
    
    @staticmethod
    def rule_based_weights(rule_results: List[Dict[str, Any]]) -> float:
        """Baseline 2: ê·œì¹™ ê¸°ë°˜ ê°€ì¤‘ì¹˜ (í˜„ì¬ ì‚¬ìš© ì¤‘)"""
        learner = RuleWeightLearner(use_ai=False)
        return learner.calculate_weighted_score(rule_results)
    
    @staticmethod
    def majority_class(y_true: List[str]) -> str:
        """Baseline 3: ë‹¤ìˆ˜ í´ë˜ìŠ¤ ë¶„ë¥˜ê¸°"""
        from collections import Counter
        return Counter(y_true).most_common(1)[0][0]
    
    @staticmethod
    def random_classifier(y_true: List[str], random_state: int = 42) -> List[str]:
        """Baseline 4: ëœë¤ ë¶„ë¥˜ê¸°"""
        np.random.seed(random_state)
        unique_labels = list(set(y_true))
        return [np.random.choice(unique_labels) for _ in y_true]


def score_to_label(score: float, threshold: float = 50.0) -> str:
    """ì ìˆ˜ë¥¼ ë¼ë²¨ë¡œ ë³€í™˜"""
    if score >= threshold:
        return "fraud"
    else:
        return "normal"


def evaluate_model(
    test_data: List[Dict[str, Any]],
    model_name: str,
    predict_fn
) -> Dict[str, float]:
    """
    ëª¨ë¸ í‰ê°€
    
    Args:
        test_data: í…ŒìŠ¤íŠ¸ ë°ì´í„°
        model_name: ëª¨ë¸ ì´ë¦„
        predict_fn: ì˜ˆì¸¡ í•¨ìˆ˜ (rule_results -> score)
    
    Returns:
        í‰ê°€ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
    """
    y_true = []
    y_pred = []
    y_pred_scores = []
    
    for item in test_data:
        rule_results = item.get("rule_results", [])
        actual_label = item.get("ground_truth_label", "normal")
        actual_score = item.get("actual_risk_score", 0.0)
        
        # ì˜ˆì¸¡
        try:
            predicted_score = predict_fn(rule_results)
            predicted_label = score_to_label(predicted_score)
        except Exception as e:
            print(f"âš ï¸  ì˜ˆì¸¡ ì—ëŸ¬ ({model_name}): {e}")
            predicted_score = 0.0
            predicted_label = "normal"
        
        y_true.append(actual_label)
        y_pred.append(predicted_label)
        y_pred_scores.append(predicted_score)
    
    # ë¼ë²¨ì„ ìˆ«ìë¡œ ë³€í™˜ (fraud=1, normal=0)
    y_true_binary = [1 if label == "fraud" else 0 for label in y_true]
    y_pred_binary = [1 if label == "fraud" else 0 for label in y_pred]
    
    # ì ìˆ˜ë¥¼ í™•ë¥ ë¡œ ë³€í™˜ (0~100 -> 0~1)
    y_pred_proba = [score / 100.0 for score in y_pred_scores]
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    metrics = {
        "model_name": model_name,
        "accuracy": accuracy_score(y_true_binary, y_pred_binary),
        "precision": precision_score(y_true_binary, y_pred_binary, zero_division=0),
        "recall": recall_score(y_true_binary, y_pred_binary, zero_division=0),
        "f1_score": f1_score(y_true_binary, y_pred_binary, zero_division=0),
    }
    
    # AUC ê³„ì‚° (ì ìˆ˜ê°€ í•„ìš”í•œ ê²½ìš°)
    try:
        if len(set(y_pred_proba)) > 1:  # ëª¨ë“  ê°’ì´ ê°™ì§€ ì•Šì€ ê²½ìš°
            metrics["roc_auc"] = roc_auc_score(y_true_binary, y_pred_proba)
            metrics["average_precision"] = average_precision_score(y_true_binary, y_pred_proba)
        else:
            metrics["roc_auc"] = 0.5  # ëœë¤ ìˆ˜ì¤€
            metrics["average_precision"] = 0.0
    except Exception as e:
        metrics["roc_auc"] = 0.5
        metrics["average_precision"] = 0.0
    
    # Confusion Matrix
    cm = confusion_matrix(y_true_binary, y_pred_binary)
    metrics["confusion_matrix"] = {
        "true_negative": int(cm[0][0]),
        "false_positive": int(cm[0][1]),
        "false_negative": int(cm[1][0]),
        "true_positive": int(cm[1][1])
    }
    
    return metrics


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    test_path = project_root / "data" / "dataset" / "test.json"
    if not test_path.exists():
        print(f"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤: {test_path}")
        print("   ë¨¼ì € ë°ì´í„°ì…‹ì„ ë¶„í• í•˜ì„¸ìš”: python scripts/split_dataset.py")
        return
    
    print("=" * 60)
    print("ëª¨ë¸ í‰ê°€ ì‹œì‘")
    print("=" * 60)
    
    print(f"\nğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ: {test_path.name}")
    try:
        with open(test_path, 'r') as f:
            test_data = json.load(f)
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(test_data)}ê°œ")
    
    # ë¼ë²¨ ë¶„í¬ í™•ì¸
    labels = [item.get("ground_truth_label", "unknown") for item in test_data]
    fraud_count = labels.count("fraud")
    normal_count = labels.count("normal")
    print(f"   Fraud: {fraud_count}ê°œ ({fraud_count/len(test_data)*100:.1f}%)")
    print(f"   Normal: {normal_count}ê°œ ({normal_count/len(test_data)*100:.1f}%)")
    
    # Baseline ëª¨ë¸ë“¤ í‰ê°€
    print("\n" + "=" * 60)
    print("Baseline ëª¨ë¸ í‰ê°€")
    print("=" * 60)
    
    baseline_results = []
    
    # Baseline 1: ë‹¨ìˆœ í•©ì‚°
    print("\n1ï¸âƒ£  Baseline: ë‹¨ìˆœ í•©ì‚° (Simple Sum)")
    result1 = evaluate_model(test_data, "Simple Sum", BaselineModels.simple_sum)
    baseline_results.append(result1)
    print(f"   Accuracy: {result1['accuracy']:.4f}")
    print(f"   Precision: {result1['precision']:.4f}")
    print(f"   Recall: {result1['recall']:.4f}")
    print(f"   F1-Score: {result1['f1_score']:.4f}")
    print(f"   ROC-AUC: {result1['roc_auc']:.4f}")
    print(f"   Average Precision: {result1['average_precision']:.4f}")
    
    # Baseline 2: ê·œì¹™ ê¸°ë°˜ ê°€ì¤‘ì¹˜
    print("\n2ï¸âƒ£  Baseline: ê·œì¹™ ê¸°ë°˜ ê°€ì¤‘ì¹˜ (Rule-based Weights)")
    result2 = evaluate_model(test_data, "Rule-based", BaselineModels.rule_based_weights)
    baseline_results.append(result2)
    print(f"   Accuracy: {result2['accuracy']:.4f}")
    print(f"   Precision: {result2['precision']:.4f}")
    print(f"   Recall: {result2['recall']:.4f}")
    print(f"   F1-Score: {result2['f1_score']:.4f}")
    print(f"   ROC-AUC: {result2['roc_auc']:.4f}")
    print(f"   Average Precision: {result2['average_precision']:.4f}")
    
    # Baseline 3: ë‹¤ìˆ˜ í´ë˜ìŠ¤ ë¶„ë¥˜ê¸°
    print("\n3ï¸âƒ£  Baseline: ë‹¤ìˆ˜ í´ë˜ìŠ¤ ë¶„ë¥˜ê¸° (Majority Class)")
    y_true_labels = [item.get("ground_truth_label", "normal") for item in test_data]
    majority_label = BaselineModels.majority_class(y_true_labels)
    y_pred_majority = [majority_label] * len(test_data)
    
    y_true_binary = [1 if label == "fraud" else 0 for label in y_true_labels]
    y_pred_binary = [1 if label == "fraud" else 0 for label in y_pred_majority]
    
    result3 = {
        "model_name": "Majority Class",
        "accuracy": accuracy_score(y_true_binary, y_pred_binary),
        "precision": precision_score(y_true_binary, y_pred_binary, zero_division=0),
        "recall": recall_score(y_true_binary, y_pred_binary, zero_division=0),
        "f1_score": f1_score(y_true_binary, y_pred_binary, zero_division=0),
        "roc_auc": 0.5,  # ë‹¤ìˆ˜ í´ë˜ìŠ¤ëŠ” AUC ì˜ë¯¸ ì—†ìŒ
        "average_precision": 0.0,
    }
    baseline_results.append(result3)
    print(f"   Accuracy: {result3['accuracy']:.4f}")
    print(f"   Precision: {result3['precision']:.4f}")
    print(f"   Recall: {result3['recall']:.4f}")
    print(f"   F1-Score: {result3['f1_score']:.4f}")
    print(f"   (Majority class: {majority_label})")
    
    # Baseline 4: ëœë¤ ë¶„ë¥˜ê¸°
    print("\n4ï¸âƒ£  Baseline: ëœë¤ ë¶„ë¥˜ê¸° (Random)")
    y_pred_random = BaselineModels.random_classifier(y_true_labels)
    y_pred_binary_random = [1 if label == "fraud" else 0 for label in y_pred_random]
    
    result4 = {
        "model_name": "Random",
        "accuracy": accuracy_score(y_true_binary, y_pred_binary_random),
        "precision": precision_score(y_true_binary, y_pred_binary_random, zero_division=0),
        "recall": recall_score(y_true_binary, y_pred_binary_random, zero_division=0),
        "f1_score": f1_score(y_true_binary, y_pred_binary_random, zero_division=0),
        "roc_auc": 0.5,  # ëœë¤ì€ AUC 0.5
        "average_precision": fraud_count / len(test_data),  # ëœë¤ ì˜ˆìƒê°’
    }
    baseline_results.append(result4)
    print(f"   Accuracy: {result4['accuracy']:.4f}")
    print(f"   Precision: {result4['precision']:.4f}")
    print(f"   Recall: {result4['recall']:.4f}")
    print(f"   F1-Score: {result4['f1_score']:.4f}")
    
    # AI ëª¨ë¸ í‰ê°€
    print("\n" + "=" * 60)
    print("AI ëª¨ë¸ í‰ê°€")
    print("=" * 60)
    
    model_path = project_root / "models" / "rule_weights.pkl"
    if not model_path.exists():
        print(f"\nâš ï¸  AI ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        print("   ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”: python scripts/train_ai_model.py")
    else:
        print(f"\nğŸ“¦ AI ëª¨ë¸ ë¡œë“œ: {model_path.name}")
        try:
            with open(model_path, 'rb') as f:
                model_data = pickle.load(f)
            
            use_ai = model_data.get("use_ai", False) or model_data.get("model") is not None
            
            if use_ai and model_data.get("model"):
                learner = RuleWeightLearner(use_ai=True)
                learner.model = model_data.get("model")
                learner.scaler = model_data.get("scaler")
                learner.rule_features = model_data.get("rule_features", {})
                
                def ai_predict(rule_results):
                    return learner.calculate_weighted_score(rule_results)
                
                print("\n5ï¸âƒ£  AI ëª¨ë¸: GradientBoostingClassifier")
                result5 = evaluate_model(test_data, "AI Model", ai_predict)
                print(f"   Accuracy: {result5['accuracy']:.4f}")
                print(f"   Precision: {result5['precision']:.4f}")
                print(f"   Recall: {result5['recall']:.4f}")
                print(f"   F1-Score: {result5['f1_score']:.4f}")
                print(f"   ROC-AUC: {result5['roc_auc']:.4f}")
                print(f"   Average Precision: {result5['average_precision']:.4f}")
            else:
                print("\nâš ï¸  ì €ì¥ëœ ëª¨ë¸ì´ ê·œì¹™ ê¸°ë°˜ ê°€ì¤‘ì¹˜ì…ë‹ˆë‹¤.")
                print("   AI ëª¨ë¸ì„ í•™ìŠµí•˜ë ¤ë©´: python scripts/train_ai_model.py")
        except Exception as e:
            print(f"\nâŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    # ê²°ê³¼ ë¹„êµ í…Œì´ë¸”
    print("\n" + "=" * 60)
    print("ğŸ“Š ê²°ê³¼ ë¹„êµ")
    print("=" * 60)
    
    print(f"\n{'ëª¨ë¸':<25} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'ROC-AUC':<10}")
    print("-" * 75)
    
    for result in baseline_results:
        print(f"{result['model_name']:<25} "
              f"{result['accuracy']:<10.4f} "
              f"{result['precision']:<10.4f} "
              f"{result['recall']:<10.4f} "
              f"{result['f1_score']:<10.4f} "
              f"{result['roc_auc']:<10.4f}")
    
    # ê²°ê³¼ ì €ì¥
    output_path = project_root / "data" / "dataset" / "evaluation_results.json"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    all_results = {
        "test_samples": len(test_data),
        "label_distribution": {
            "fraud": fraud_count,
            "normal": normal_count
        },
        "baseline_results": baseline_results
    }
    
    with open(output_path, 'w') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥: {output_path}")
    print("\nâœ… í‰ê°€ ì™„ë£Œ!")


if __name__ == "__main__":
    main()

