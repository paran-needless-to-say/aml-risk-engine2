#!/usr/bin/env python3
"""
MPOCryptoML ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ MPOCryptoML ëª¨ë¸ê³¼ Baseline ëª¨ë¸ë“¤ì„ í‰ê°€ ë° ë¹„êµ

ì‚¬ìš©ë²•:
    python scripts/evaluate_mpocryptml_model.py
"""
import sys
import json
import pickle
from pathlib import Path
from typing import Dict, List, Any, Tuple
import numpy as np
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score, confusion_matrix,
    classification_report
)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.scoring.ai_weight_learner import RuleWeightLearner


def extract_features(item: Dict[str, Any]) -> np.ndarray:
    """
    MPOCryptoML í”¼ì²˜ ì¶”ì¶œ (í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼)
    
    Args:
        item: ë°ì´í„°ì…‹ í•­ëª©
    
    Returns:
        í”¼ì²˜ ë²¡í„° (numpy array)
    """
    ml_features = item.get("ml_features", {})
    
    # ìˆ«ìí˜• í”¼ì²˜ ì¶”ì¶œ
    features = [
        ml_features.get("ppr_score", 0.0),
        ml_features.get("sdn_ppr", 0.0),
        ml_features.get("mixer_ppr", 0.0),
        ml_features.get("pattern_score", 0.0),
        ml_features.get("n_theta", 0.0),
        ml_features.get("n_omega", 0.0),
        ml_features.get("fan_in_count", 0),
        ml_features.get("fan_out_count", 0),
        ml_features.get("gather_scatter", 0.0),
        ml_features.get("graph_nodes", 0),
        ml_features.get("graph_edges", 0),
    ]
    
    # íŒ¨í„´ í”¼ì²˜ (one-hot encoding)
    detected_patterns = ml_features.get("detected_patterns", [])
    pattern_types = ["fan_in", "fan_out", "gather_scatter", "stack", "bipartite"]
    for pattern_type in pattern_types:
        features.append(1.0 if pattern_type in detected_patterns else 0.0)
    
    # Rule-based ì ìˆ˜ë„ í”¼ì²˜ë¡œ í¬í•¨
    rule_score = item.get("rule_score", 0.0)
    features.append(rule_score)
    
    return np.array(features, dtype=np.float32)


class BaselineModels:
    """Baseline ëª¨ë¸ë“¤"""
    
    @staticmethod
    def simple_sum(item: Dict[str, Any]) -> float:
        """Baseline 1: Rule-based ë‹¨ìˆœ í•©ì‚°"""
        rule_results = item.get("rule_results", [])
        return min(100.0, sum(r.get("score", 0) for r in rule_results))
    
    @staticmethod
    def rule_based_weights(item: Dict[str, Any]) -> float:
        """Baseline 2: ê·œì¹™ ê¸°ë°˜ ê°€ì¤‘ì¹˜"""
        rule_results = item.get("rule_results", [])
        learner = RuleWeightLearner(use_ai=False)
        return learner.calculate_weighted_score(rule_results)
    
    @staticmethod
    def rule_score_only(item: Dict[str, Any]) -> float:
        """Baseline 3: Rule Scoreë§Œ ì‚¬ìš©"""
        return item.get("rule_score", 0.0)
    
    @staticmethod
    def ml_features_only(item: Dict[str, Any]) -> float:
        """Baseline 4: MPOCryptoML í”¼ì²˜ë§Œ ì‚¬ìš© (ê°€ì¤‘ í‰ê· )"""
        ml_features = item.get("ml_features", {})
        ppr = ml_features.get("ppr_score", 0.0) * 100
        pattern = ml_features.get("pattern_score", 0.0)
        n_theta = ml_features.get("n_theta", 0.0) * 100
        n_omega = ml_features.get("n_omega", 0.0) * 100
        
        # ê°„ë‹¨í•œ ê°€ì¤‘ í‰ê· 
        score = (ppr * 0.3 + pattern * 0.4 + n_theta * 0.15 + n_omega * 0.15)
        return min(100.0, max(0.0, score))
    
    @staticmethod
    def majority_class(y_true: List[str]) -> str:
        """Baseline 5: ë‹¤ìˆ˜ í´ë˜ìŠ¤ ë¶„ë¥˜ê¸°"""
        from collections import Counter
        return Counter(y_true).most_common(1)[0][0]
    
    @staticmethod
    def random_classifier(y_true: List[str], random_state: int = 42) -> List[str]:
        """Baseline 6: ëœë¤ ë¶„ë¥˜ê¸°"""
        np.random.seed(random_state)
        unique_labels = list(set(y_true))
        return [np.random.choice(unique_labels) for _ in y_true]


def score_to_label(score: float, threshold: float = 50.0) -> str:
    """ì ìˆ˜ë¥¼ ë¼ë²¨ë¡œ ë³€í™˜"""
    if score >= threshold:
        return "fraud"
    else:
        return "normal"


def evaluate_model(
    test_data: List[Dict[str, Any]],
    model_name: str,
    predict_fn
) -> Dict[str, Any]:
    """
    ëª¨ë¸ í‰ê°€
    
    Args:
        test_data: í…ŒìŠ¤íŠ¸ ë°ì´í„°
        model_name: ëª¨ë¸ ì´ë¦„
        predict_fn: ì˜ˆì¸¡ í•¨ìˆ˜ (item -> score ë˜ëŠ” item -> label)
    
    Returns:
        í‰ê°€ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
    """
    y_true = []
    y_pred = []
    y_pred_scores = []
    
    for item in test_data:
        actual_label = item.get("ground_truth_label", "normal")
        
        # ì˜ˆì¸¡
        try:
            prediction = predict_fn(item)
            
            # ì ìˆ˜ì¸ì§€ ë¼ë²¨ì¸ì§€ í™•ì¸
            if isinstance(prediction, (int, float)):
                predicted_score = float(prediction)
                predicted_label = score_to_label(predicted_score)
            else:
                predicted_label = str(prediction)
                # ë¼ë²¨ë§Œ ìˆëŠ” ê²½ìš° ì ìˆ˜ ì¶”ì •
                predicted_score = 85.0 if predicted_label == "fraud" else 15.0
        except Exception as e:
            print(f"âš ï¸  ì˜ˆì¸¡ ì—ëŸ¬ ({model_name}): {e}")
            predicted_score = 0.0
            predicted_label = "normal"
        
        y_true.append(actual_label)
        y_pred.append(predicted_label)
        y_pred_scores.append(predicted_score)
    
    # ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜
    y_true_binary = [1 if label == "fraud" else 0 for label in y_true]
    y_pred_binary = [1 if label == "fraud" else 0 for label in y_pred]
    y_pred_proba = [score / 100.0 for score in y_pred_scores]
    
    # í‰ê°€ ì§€í‘œ ê³„ì‚°
    accuracy = accuracy_score(y_true_binary, y_pred_binary)
    precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)
    recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)
    f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)
    
    # ROC-AUC ë° Average Precision
    try:
        if len(set(y_pred_proba)) > 1:
            roc_auc = roc_auc_score(y_true_binary, y_pred_proba)
            avg_precision = average_precision_score(y_true_binary, y_pred_proba)
        else:
            roc_auc = 0.5
            avg_precision = 0.0
    except Exception as e:
        roc_auc = 0.5
        avg_precision = 0.0
    
    # Confusion Matrix
    cm = confusion_matrix(y_true_binary, y_pred_binary)
    
    metrics = {
        "model_name": model_name,
        "accuracy": float(accuracy),
        "precision": float(precision),
        "recall": float(recall),
        "f1_score": float(f1),
        "roc_auc": float(roc_auc),
        "average_precision": float(avg_precision),
        "confusion_matrix": {
            "true_negative": int(cm[0][0]),
            "false_positive": int(cm[0][1]),
            "false_negative": int(cm[1][0]),
            "true_positive": int(cm[1][1])
        }
    }
    
    return metrics


def evaluate_mpocryptml_model(
    test_data: List[Dict[str, Any]],
    model_path: Path
) -> Dict[str, Any]:
    """
    MPOCryptoML ëª¨ë¸ í‰ê°€
    
    Args:
        test_data: í…ŒìŠ¤íŠ¸ ë°ì´í„°
        model_path: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
    
    Returns:
        í‰ê°€ ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬
    """
    # ëª¨ë¸ ë¡œë“œ
    with open(model_path, 'rb') as f:
        model_data = pickle.load(f)
    
    model = model_data["model"]
    scaler = model_data["scaler"]
    
    # í”¼ì²˜ ì¶”ì¶œ ë° ìŠ¤ì¼€ì¼ë§
    X_test = []
    for item in test_data:
        features = extract_features(item)
        X_test.append(features)
    
    X_test = np.array(X_test)
    X_test_scaled = scaler.transform(X_test)
    
    # ì˜ˆì¸¡
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    y_pred = model.predict(X_test_scaled)
    
    # ë¼ë²¨ ë³€í™˜
    y_true = []
    y_pred_labels = []
    y_pred_scores = []
    
    for i, item in enumerate(test_data):
        actual_label = item.get("ground_truth_label", "normal")
        predicted_binary = y_pred[i]
        predicted_label = "fraud" if predicted_binary == 1 else "normal"
        predicted_score = y_pred_proba[i] * 100.0
        
        y_true.append(actual_label)
        y_pred_labels.append(predicted_label)
        y_pred_scores.append(predicted_score)
    
    # ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜
    y_true_binary = [1 if label == "fraud" else 0 for label in y_true]
    y_pred_binary = [1 if label == "fraud" else 0 for label in y_pred_labels]
    
    # í‰ê°€ ì§€í‘œ ê³„ì‚°
    accuracy = accuracy_score(y_true_binary, y_pred_binary)
    precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)
    recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)
    f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)
    
    try:
        if len(set(y_pred_proba)) > 1:
            roc_auc = roc_auc_score(y_true_binary, y_pred_proba)
            avg_precision = average_precision_score(y_true_binary, y_pred_proba)
        else:
            roc_auc = 0.5
            avg_precision = 0.0
    except Exception as e:
        roc_auc = 0.5
        avg_precision = 0.0
    
    cm = confusion_matrix(y_true_binary, y_pred_binary)
    
    metrics = {
        "model_name": "MPOCryptoML (Logistic Regression)",
        "accuracy": float(accuracy),
        "precision": float(precision),
        "recall": float(recall),
        "f1_score": float(f1),
        "roc_auc": float(roc_auc),
        "average_precision": float(avg_precision),
        "confusion_matrix": {
            "true_negative": int(cm[0][0]),
            "false_positive": int(cm[0][1]),
            "false_negative": int(cm[1][0]),
            "true_positive": int(cm[1][1])
        }
    }
    
    return metrics


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("MPOCryptoML ëª¨ë¸ í‰ê°€")
    print("=" * 60)
    
    dataset_dir = project_root / "data" / "dataset"
    test_path = dataset_dir / "test.json"
    model_path = project_root / "models" / "mpocryptml_model.pkl"
    
    if not test_path.exists():
        print(f"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_path}")
        return
    
    if not model_path.exists():
        print(f"âŒ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        print("   ë¨¼ì € ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”: python scripts/train_mpocryptml_model.py")
        return
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
    with open(test_path, 'r') as f:
        test_data = json.load(f)
    
    print(f"   ì´ {len(test_data)}ê°œ ìƒ˜í”Œ")
    
    # ë¼ë²¨ ë¶„í¬ í™•ì¸
    labels = [item.get("ground_truth_label", "normal") for item in test_data]
    from collections import Counter
    label_counts = Counter(labels)
    print(f"   ë¼ë²¨ ë¶„í¬: {dict(label_counts)}")
    
    # ëª¨ë¸ í‰ê°€
    print("\n" + "=" * 60)
    print("ğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")
    print("=" * 60)
    
    all_results = []
    
    # 1. MPOCryptoML ëª¨ë¸
    print("\n1ï¸âƒ£  MPOCryptoML ëª¨ë¸ í‰ê°€ ì¤‘...")
    mpocryptml_results = evaluate_mpocryptml_model(test_data, model_path)
    all_results.append(mpocryptml_results)
    
    # 2. Baseline ëª¨ë¸ë“¤
    print("\n2ï¸âƒ£  Baseline ëª¨ë¸ë“¤ í‰ê°€ ì¤‘...")
    
    baseline_models = {
        "Simple Sum": lambda item: BaselineModels.simple_sum(item),
        "Rule-based Weights": lambda item: BaselineModels.rule_based_weights(item),
        "Rule Score Only": lambda item: BaselineModels.rule_score_only(item),
        "ML Features Only": lambda item: BaselineModels.ml_features_only(item),
    }
    
    for name, predict_fn in baseline_models.items():
        print(f"   - {name}...")
        results = evaluate_model(test_data, name, predict_fn)
        all_results.append(results)
    
    # 3. Majority Class
    print("   - Majority Class...")
    majority_label = BaselineModels.majority_class(labels)
    majority_results = evaluate_model(
        test_data,
        "Majority Class",
        lambda item: majority_label
    )
    all_results.append(majority_results)
    
    # 4. Random Classifier
    print("   - Random Classifier...")
    random_labels = BaselineModels.random_classifier(labels)
    random_results = evaluate_model(
        test_data,
        "Random Classifier",
        lambda item, idx=iter(range(len(test_data))): random_labels[next(idx)]
    )
    all_results.append(random_results)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“ˆ í‰ê°€ ê²°ê³¼")
    print("=" * 60)
    
    # í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ì¶œë ¥
    print(f"\n{'ëª¨ë¸':<30} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'ROC-AUC':<10}")
    print("-" * 80)
    
    for results in all_results:
        name = results["model_name"]
        acc = results["accuracy"]
        prec = results["precision"]
        rec = results["recall"]
        f1 = results["f1_score"]
        auc = results["roc_auc"]
        
        print(f"{name:<30} {acc:<10.4f} {prec:<10.4f} {rec:<10.4f} {f1:<10.4f} {auc:<10.4f}")
    
    # ìƒì„¸ ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“‹ ìƒì„¸ ê²°ê³¼")
    print("=" * 60)
    
    for results in all_results:
        print(f"\nğŸ”¹ {results['model_name']}:")
        print(f"   Accuracy:        {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)")
        print(f"   Precision:       {results['precision']:.4f}")
        print(f"   Recall:          {results['recall']:.4f}")
        print(f"   F1-Score:        {results['f1_score']:.4f}")
        print(f"   ROC-AUC:         {results['roc_auc']:.4f}")
        print(f"   Average Precision: {results['average_precision']:.4f}")
        cm = results['confusion_matrix']
        print(f"   Confusion Matrix:")
        print(f"      TN: {cm['true_negative']}, FP: {cm['false_positive']}")
        print(f"      FN: {cm['false_negative']}, TP: {cm['true_positive']}")
    
    # ê²°ê³¼ ì €ì¥
    output_path = dataset_dir / "mpocryptml_evaluation_results.json"
    with open(output_path, 'w') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥: {output_path}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
    best_model = max(all_results, key=lambda x: x['f1_score'])
    print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ (F1-Score ê¸°ì¤€): {best_model['model_name']}")
    print(f"   F1-Score: {best_model['f1_score']:.4f}")
    print(f"   Accuracy: {best_model['accuracy']:.4f}")


if __name__ == "__main__":
    main()

