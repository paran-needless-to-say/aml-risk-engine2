#!/usr/bin/env python3
"""
ë£° ìµœì í™” ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸

1. ë£°ë³„ íš¨ê³¼ ì¸¡ì •
2. ì¶•ë³„ ì¤‘ìš”ë„ ì¸¡ì •
3. ë£° ì œê±°/ìˆ˜ì • ì‹¤í—˜
4. ì„ê³„ê°’ ìµœì í™”
"""
import sys
import json
import yaml
from pathlib import Path
from typing import Dict, List, Any, Set
from collections import defaultdict
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.scoring.stage1_scorer import Stage1Scorer
from core.rules.evaluator import RuleEvaluator


def analyze_rule_effectiveness(
    test_data: List[Dict[str, Any]],
    rules_path: str = "rules/tracex_rules.yaml"
) -> Dict[str, Any]:
    """ë£°ë³„ íš¨ê³¼ ë¶„ì„"""
    print("=" * 80)
    print("ë£°ë³„ íš¨ê³¼ ë¶„ì„")
    print("=" * 80)
    
    rule_evaluator = RuleEvaluator(rules_path)
    
    # ë£°ë³„ í†µê³„
    rule_stats = defaultdict(lambda: {
        "fired_count": 0,
        "fraud_when_fired": 0,
        "normal_when_fired": 0,
        "total_score": 0.0
    })
    
    # ì¶•ë³„ í†µê³„
    axis_stats = defaultdict(lambda: {
        "fired_count": 0,
        "fraud_when_fired": 0,
        "normal_when_fired": 0
    })
    
    print("\nğŸ“Š ë£° ë°œë™ í†µê³„ ìˆ˜ì§‘ ì¤‘...")
    for item in test_data:
        label = item.get("ground_truth_label", "normal")
        is_fraud = (label == "fraud")
        
        tx_data = {
            "from": item.get("from", ""),
            "to": item.get("to", ""),
            "usd_value": item.get("usd_value", 0),
            "timestamp": item.get("timestamp", 0),
            "tx_hash": item.get("tx_hash", ""),
            "chain": item.get("chain", "ethereum"),
            "is_sanctioned": item.get("tx_context", {}).get("is_sanctioned", False),
            "is_mixer": item.get("tx_context", {}).get("is_mixer", False),
        }
        
        rule_results = rule_evaluator.evaluate_single_transaction(tx_data)
        
        for rule in rule_results:
            rule_id = rule.get("rule_id", "")
            axis = rule.get("axis", "B")
            score = rule.get("score", 0.0)
            
            # ë£°ë³„ í†µê³„
            rule_stats[rule_id]["fired_count"] += 1
            rule_stats[rule_id]["total_score"] += score
            if is_fraud:
                rule_stats[rule_id]["fraud_when_fired"] += 1
            else:
                rule_stats[rule_id]["normal_when_fired"] += 1
            
            # ì¶•ë³„ í†µê³„
            axis_stats[axis]["fired_count"] += 1
            if is_fraud:
                axis_stats[axis]["fraud_when_fired"] += 1
            else:
                axis_stats[axis]["normal_when_fired"] += 1
    
    # ë£°ë³„ íš¨ê³¼ ê³„ì‚°
    rule_effectiveness = {}
    for rule_id, stats in rule_stats.items():
        total_fired = stats["fired_count"]
        if total_fired > 0:
            fraud_ratio = stats["fraud_when_fired"] / total_fired
            avg_score = stats["total_score"] / total_fired
            rule_effectiveness[rule_id] = {
                "fired_count": total_fired,
                "fraud_ratio": fraud_ratio,
                "fraud_when_fired": stats["fraud_when_fired"],
                "normal_when_fired": stats["normal_when_fired"],
                "avg_score": avg_score,
                "effectiveness": fraud_ratio * avg_score  # íš¨ê³¼ì„± ì ìˆ˜
            }
    
    # ì¶•ë³„ íš¨ê³¼ ê³„ì‚°
    axis_effectiveness = {}
    for axis, stats in axis_stats.items():
        total_fired = stats["fired_count"]
        if total_fired > 0:
            fraud_ratio = stats["fraud_when_fired"] / total_fired
            axis_effectiveness[axis] = {
                "fired_count": total_fired,
                "fraud_ratio": fraud_ratio,
                "fraud_when_fired": stats["fraud_when_fired"],
                "normal_when_fired": stats["normal_when_fired"]
            }
    
    return {
        "rule_effectiveness": rule_effectiveness,
        "axis_effectiveness": axis_effectiveness
    }


def test_rule_removal(
    test_data: List[Dict[str, Any]],
    rules_to_remove: Set[str],
    rules_path: str = "rules/tracex_rules.yaml"
) -> Dict[str, float]:
    """íŠ¹ì • ë£° ì œê±° ì‹œ ì„±ëŠ¥ ì¸¡ì •"""
    # ë£° íŒŒì¼ ë¡œë“œ
    with open(rules_path, 'r') as f:
        rules_config = yaml.safe_load(f)
    
    # ë£° ì œê±°
    original_rules = rules_config["rules"]
    filtered_rules = [r for r in original_rules if r["id"] not in rules_to_remove]
    rules_config["rules"] = filtered_rules
    
    # ì„ì‹œ ë£° íŒŒì¼ ìƒì„±
    temp_rules_path = project_root / "rules" / "temp_rules.yaml"
    with open(temp_rules_path, 'w') as f:
        yaml.dump(rules_config, f)
    
    try:
        # Stage 1 ìŠ¤ì½”ì–´ëŸ¬ë¡œ í‰ê°€
        stage1_scorer = Stage1Scorer(rules_path=str(temp_rules_path))
        
        y_true = []
        y_pred_scores = []
        
        for item in test_data:
            label = item.get("ground_truth_label", "normal")
            y_true.append(1 if label == "fraud" else 0)
            
            tx_data = {
                "from": item.get("from", ""),
                "to": item.get("to", ""),
                "usd_value": item.get("usd_value", 0),
                "timestamp": item.get("timestamp", 0),
                "tx_hash": item.get("tx_hash", ""),
                "chain": item.get("chain", "ethereum"),
                "is_sanctioned": item.get("tx_context", {}).get("is_sanctioned", False),
                "is_mixer": item.get("tx_context", {}).get("is_mixer", False),
            }
            
            ml_features = item.get("ml_features", {})
            tx_context = item.get("tx_context", {})
            
            result = stage1_scorer.calculate_risk_score(tx_data, ml_features, tx_context)
            y_pred_scores.append(result["risk_score"])
        
        # Threshold ìµœì í™”
        best_threshold = 50.0
        best_f1 = 0.0
        
        for threshold in range(10, 90, 2):
            y_pred = [1 if s >= threshold else 0 for s in y_pred_scores]
            f1 = f1_score(y_true, y_pred, zero_division=0)
            if f1 > best_f1:
                best_f1 = f1
                best_threshold = threshold
        
        y_pred = [1 if s >= best_threshold else 0 for s in y_pred_scores]
        
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)
        f1 = f1_score(y_true, y_pred, zero_division=0)
        
        return {
            "accuracy": accuracy,
            "precision": precision,
            "recall": recall,
            "f1_score": f1,
            "threshold": best_threshold
        }
    finally:
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        if temp_rules_path.exists():
            temp_rules_path.unlink()


def optimize_axis_weights(
    test_data: List[Dict[str, Any]],
    rules_path: str = "rules/tracex_rules.yaml"
) -> Dict[str, Any]:
    """ì¶•ë³„ ê°€ì¤‘ì¹˜ ìµœì í™”"""
    print("=" * 80)
    print("ì¶•ë³„ ê°€ì¤‘ì¹˜ ìµœì í™”")
    print("=" * 80)
    
    from core.scoring.improved_rule_scorer import ImprovedRuleScorer
    from core.rules.evaluator import RuleEvaluator
    
    rule_evaluator = RuleEvaluator(rules_path)
    
    # ê°€ì¤‘ì¹˜ ì¡°í•© í…ŒìŠ¤íŠ¸
    weight_combinations = [
        {"B": 1.0, "C": 1.0, "E": 1.0},  # ê· ë“±
        {"B": 1.2, "C": 1.3, "E": 1.4},  # í˜„ì¬
        {"B": 1.0, "C": 1.5, "E": 1.5},  # C, E ê°•ì¡°
        {"B": 1.5, "C": 1.0, "E": 1.0},  # B ê°•ì¡°
        {"B": 1.0, "C": 1.0, "E": 1.5},  # E ê°•ì¡°
        {"B": 1.3, "C": 1.3, "E": 1.3},  # ê· ë“± ì¦ê°€
    ]
    
    best_f1 = -1.0
    best_weights = None
    best_results = None
    
    print("\nğŸ” ê°€ì¤‘ì¹˜ ì¡°í•© í…ŒìŠ¤íŠ¸ ì¤‘...")
    for weights in weight_combinations:
        # ImprovedRuleScorerì— ê°€ì¤‘ì¹˜ ì ìš© (ìˆ˜ë™ìœ¼ë¡œ)
        # ì‹¤ì œë¡œëŠ” ImprovedRuleScorerë¥¼ ìˆ˜ì •í•´ì•¼ í•¨
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ í…ŒìŠ¤íŠ¸
        
        scorer = ImprovedRuleScorer(use_axis_bonus=True)
        # ê°€ì¤‘ì¹˜ë¥¼ ì§ì ‘ ìˆ˜ì •í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, ê²°ê³¼ë§Œ ì¶œë ¥
        
        y_true = []
        y_pred_scores = []
        
        for item in test_data:
            label = item.get("ground_truth_label", "normal")
            y_true.append(1 if label == "fraud" else 0)
            
            tx_data = {
                "from": item.get("from", ""),
                "to": item.get("to", ""),
                "usd_value": item.get("usd_value", 0),
                "timestamp": item.get("timestamp", 0),
                "tx_hash": item.get("tx_hash", ""),
                "chain": item.get("chain", "ethereum"),
                "is_sanctioned": item.get("tx_context", {}).get("is_sanctioned", False),
                "is_mixer": item.get("tx_context", {}).get("is_mixer", False),
            }
            
            rule_results = rule_evaluator.evaluate_single_transaction(tx_data)
            
            # ê°€ì¤‘ì¹˜ ì ìš© (ê°„ë‹¨í•œ ë°©ë²•)
            weighted_score = 0.0
            for rule in rule_results:
                axis = rule.get("axis", "B")
                score = rule.get("score", 0.0)
                weight = weights.get(axis, 1.0)
                weighted_score += score * weight
            
            y_pred_scores.append(min(100.0, weighted_score))
        
        # Threshold ìµœì í™”
        best_threshold = 50.0
        best_f1_local = 0.0
        
        for threshold in range(10, 90, 2):
            y_pred = [1 if s >= threshold else 0 for s in y_pred_scores]
            f1 = f1_score(y_true, y_pred, zero_division=0)
            if f1 > best_f1_local:
                best_f1_local = f1
                best_threshold = threshold
        
        y_pred = [1 if s >= best_threshold else 0 for s in y_pred_scores]
        
        accuracy = accuracy_score(y_true, y_pred)
        precision = precision_score(y_true, y_pred, zero_division=0)
        recall = recall_score(y_true, y_pred, zero_division=0)
        f1 = f1_score(y_true, y_pred, zero_division=0)
        
        print(f"\nê°€ì¤‘ì¹˜: B={weights.get('B', 1.0):.1f}, C={weights.get('C', 1.0):.1f}, E={weights.get('E', 1.0):.1f}")
        print(f"  F1: {f1:.4f}, Acc: {accuracy:.4f}, Prec: {precision:.4f}, Rec: {recall:.4f}")
        
        if f1 > best_f1:
            best_f1 = f1
            best_weights = weights
            best_results = {
                "accuracy": accuracy,
                "precision": precision,
                "recall": recall,
                "f1_score": f1,
                "threshold": best_threshold
            }
    
    print(f"\nâœ… ìµœì  ê°€ì¤‘ì¹˜:")
    print(f"   B: {best_weights.get('B', 1.0):.1f}")
    print(f"   C: {best_weights.get('C', 1.0):.1f}")
    print(f"   E: {best_weights.get('E', 1.0):.1f}")
    print(f"   F1-Score: {best_results['f1_score']:.4f}")
    
    return {
        "best_weights": best_weights,
        "results": best_results
    }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    dataset_dir = project_root / "data" / "dataset"
    test_path = dataset_dir / "test.json"
    
    if not test_path.exists():
        print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("ğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
    with open(test_path, 'r') as f:
        test_data = json.load(f)
    
    print(f"   í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(test_data)}ê°œ")
    
    # 1. ë£°ë³„ íš¨ê³¼ ë¶„ì„
    print("\n" + "=" * 80)
    effectiveness = analyze_rule_effectiveness(test_data)
    
    print("\nğŸ“Š ë£°ë³„ íš¨ê³¼ (ìƒìœ„ 10ê°œ):")
    rule_eff = effectiveness["rule_effectiveness"]
    sorted_rules = sorted(rule_eff.items(), key=lambda x: x[1]["effectiveness"], reverse=True)
    
    for rule_id, stats in sorted_rules[:10]:
        print(f"   {rule_id}: ë°œë™ {stats['fired_count']}íšŒ, "
              f"Fraud ë¹„ìœ¨ {stats['fraud_ratio']:.2%}, "
              f"íš¨ê³¼ì„± {stats['effectiveness']:.2f}")
    
    print("\nğŸ“Š ì¶•ë³„ íš¨ê³¼:")
    axis_eff = effectiveness["axis_effectiveness"]
    for axis, stats in sorted(axis_eff.items(), key=lambda x: x[1]["fraud_ratio"], reverse=True):
        print(f"   {axis}: ë°œë™ {stats['fired_count']}íšŒ, "
              f"Fraud ë¹„ìœ¨ {stats['fraud_ratio']:.2%}")
    
    # 2. íš¨ê³¼ ì—†ëŠ” ë£° ì œê±° ì‹¤í—˜
    print("\n" + "=" * 80)
    print("íš¨ê³¼ ì—†ëŠ” ë£° ì œê±° ì‹¤í—˜")
    print("=" * 80)
    
    # íš¨ê³¼ì„±ì´ ë‚®ì€ ë£° ì°¾ê¸° (ë°œë™ íšŸìˆ˜ ì ê³ , Fraud ë¹„ìœ¨ ë‚®ìŒ)
    ineffective_rules = []
    for rule_id, stats in rule_eff.items():
        if stats["fired_count"] < 10 or stats["fraud_ratio"] < 0.3:
            ineffective_rules.append(rule_id)
    
    if ineffective_rules:
        print(f"\níš¨ê³¼ì„±ì´ ë‚®ì€ ë£°: {ineffective_rules}")
        print("ì´ ë£°ë“¤ì„ ì œê±°í•˜ê³  ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        
        removal_results = test_rule_removal(test_data, set(ineffective_rules))
        print(f"\në£° ì œê±° í›„ ì„±ëŠ¥:")
        print(f"   Accuracy: {removal_results['accuracy']:.4f}")
        print(f"   F1-Score: {removal_results['f1_score']:.4f}")
    else:
        print("\níš¨ê³¼ì„±ì´ ë‚®ì€ ë£°ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # 3. ì¶•ë³„ ê°€ì¤‘ì¹˜ ìµœì í™”
    print("\n" + "=" * 80)
    weight_results = optimize_axis_weights(test_data)
    
    # ê²°ê³¼ ì €ì¥
    output_dir = project_root / "data" / "dataset"
    output_dir.mkdir(exist_ok=True)
    
    results = {
        "rule_effectiveness": effectiveness["rule_effectiveness"],
        "axis_effectiveness": effectiveness["axis_effectiveness"],
        "optimal_weights": weight_results["best_weights"],
        "weight_optimization_results": weight_results["results"]
    }
    
    if ineffective_rules:
        results["ineffective_rules"] = ineffective_rules
        results["removal_results"] = removal_results
    
    with open(output_dir / "rule_optimization_results.json", 'w') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_dir / 'rule_optimization_results.json'}")


if __name__ == "__main__":
    main()

