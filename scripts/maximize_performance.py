#!/usr/bin/env python3
"""
ì„±ëŠ¥ ìµœëŒ€í™” ìŠ¤í¬ë¦½íŠ¸ (ëª©í‘œ: 85% ì´ìƒ)

1. ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš© (5,000 â†’ 92,138)
2. ê³ ê¸‰ Feature Engineering
3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (GridSearch + RandomizedSearch)
4. ì•™ìƒë¸” ëª¨ë¸ (ë‹¤ì–‘í•œ ì¡°í•©)
5. Deep Learning ëª¨ë¸ (MLP)
6. Threshold ìµœì í™”
7. í´ë˜ìŠ¤ ë¶ˆê· í˜• ì²˜ë¦¬ ê°œì„ 
"""
import sys
import json
import pickle
from pathlib import Path
from typing import Dict, List, Any, Tuple
import numpy as np
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedKFold
from sklearn.ensemble import (
    GradientBoostingClassifier, RandomForestClassifier,
    VotingClassifier, StackingClassifier, AdaBoostClassifier
)
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score, make_scorer
)
from scipy.stats import randint, uniform
import warnings
warnings.filterwarnings('ignore')

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.scoring.stage1_scorer import Stage1Scorer

# XGBoost, LightGBM ì‹œë„
try:
    from xgboost import XGBClassifier
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

try:
    from lightgbm import LGBMClassifier
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False


def extract_advanced_features(
    stage1_result: Dict[str, Any],
    ml_features: Dict[str, Any],
    tx_context: Dict[str, Any]
) -> np.ndarray:
    """
    ê³ ê¸‰ Feature Engineering
    
    ê¸°ì¡´ 30ì°¨ì› + ì¶”ê°€ feature = 50+ ì°¨ì›
    """
    features = []
    
    # ===== ê¸°ì¡´ Features (30ì°¨ì›) =====
    
    # 1. Stage 1 features
    rule_score = stage1_result["rule_score"]
    graph_score = stage1_result["graph_score"]
    risk_score = stage1_result["risk_score"]
    features.extend([rule_score, graph_score, risk_score])
    
    # 2. Rule-based features
    rule_results = stage1_result.get("rule_results", [])
    features.append(len(rule_results))
    
    axes = [r.get("axis", "B") for r in rule_results]
    features.append(axes.count("A"))
    features.append(axes.count("B"))
    features.append(axes.count("C"))
    features.append(axes.count("D"))
    features.append(axes.count("E"))
    
    severities = [r.get("severity", "MEDIUM") for r in rule_results]
    features.append(severities.count("CRITICAL"))
    features.append(severities.count("HIGH"))
    features.append(severities.count("MEDIUM"))
    features.append(severities.count("LOW"))
    
    # 3. Graph statistics
    fan_in_count = ml_features.get("fan_in_count", 0)
    fan_out_count = ml_features.get("fan_out_count", 0)
    features.append(min(100, fan_in_count))
    features.append(min(100, fan_out_count))
    features.append(min(100, ml_features.get("tx_primary_fan_in_count", 0)))
    features.append(min(100, ml_features.get("tx_primary_fan_out_count", 0)))
    features.append(min(100.0, ml_features.get("pattern_score", 0.0)))
    
    avg_value = ml_features.get("avg_transaction_value", 0.0)
    max_value = ml_features.get("max_transaction_value", 0.0)
    if avg_value > 0:
        features.append(min(20.0, np.log1p(avg_value)))
    else:
        features.append(0.0)
    if max_value > 0:
        features.append(min(20.0, np.log1p(max_value)))
    else:
        features.append(0.0)
    
    graph_nodes = ml_features.get("graph_nodes", tx_context.get("graph_nodes", 0))
    num_transactions = ml_features.get("num_transactions", tx_context.get("num_transactions", 0))
    features.append(min(200, graph_nodes))
    features.append(min(200, num_transactions))
    
    # 4. PPR features
    features.append(min(1.0, ml_features.get("ppr_score", 0.0)))
    features.append(min(1.0, ml_features.get("sdn_ppr", 0.0)))
    features.append(min(1.0, ml_features.get("mixer_ppr", 0.0)))
    
    # 5. ì •ê·œí™” ì ìˆ˜
    features.append(min(1.0, max(0.0, ml_features.get("n_theta", 0.0))))
    features.append(min(1.0, max(0.0, ml_features.get("n_omega", 0.0))))
    
    # 6. íŒ¨í„´ íƒì§€
    features.append(ml_features.get("fan_in_detected", 0))
    features.append(ml_features.get("fan_out_detected", 0))
    features.append(ml_features.get("gather_scatter_detected", 0))
    
    # ===== ê³ ê¸‰ Features ì¶”ê°€ (20+ ì°¨ì›) =====
    
    # 7. ìƒí˜¸ì‘ìš© Features (ì¤‘ìš”!)
    features.append(rule_score * graph_score / 100.0)  # ê³±ì…ˆ
    features.append(rule_score / (graph_score + 1.0))  # ë¹„ìœ¨
    features.append((rule_score + graph_score) / 2.0)  # í‰ê· 
    features.append(abs(rule_score - graph_score))  # ì°¨ì´
    
    # 8. Fan-in/out ë¹„ìœ¨ ë° í†µê³„
    total_fan = fan_in_count + fan_out_count
    if total_fan > 0:
        features.append(fan_in_count / total_fan)  # Fan-in ë¹„ìœ¨
        features.append(fan_out_count / total_fan)  # Fan-out ë¹„ìœ¨
        features.append(abs(fan_in_count - fan_out_count) / total_fan)  # ë¹„ëŒ€ì¹­ì„±
    else:
        features.extend([0.0, 0.0, 0.0])
    
    # 9. ê±°ë˜ ê¸ˆì•¡ í†µê³„ (ê³ ê¸‰)
    min_value = ml_features.get("min_transaction_value", 0.0)
    total_value = ml_features.get("total_transaction_value", 0.0)
    transaction_count = ml_features.get("transaction_count", 0)
    
    if transaction_count > 0:
        features.append(total_value / transaction_count)  # í‰ê· 
        if max_value > 0 and min_value > 0:
            features.append(max_value / min_value)  # ìµœëŒ€/ìµœì†Œ ë¹„ìœ¨
            features.append((max_value - min_value) / max_value)  # ë³€ë™ì„±
        else:
            features.extend([0.0, 0.0])
        if avg_value > 0:
            features.append((max_value - avg_value) / avg_value)  # ì´ìƒì¹˜ ë¹„ìœ¨
        else:
            features.append(0.0)
    else:
        features.extend([0.0, 0.0, 0.0, 0.0])
    
    # 10. ê·¸ë˜í”„ êµ¬ì¡° Features
    graph_edges = ml_features.get("graph_edges", tx_context.get("graph_edges", 0))
    if graph_nodes > 1:
        max_edges = graph_nodes * (graph_nodes - 1)
        if max_edges > 0:
            features.append(graph_edges / max_edges)  # ê·¸ë˜í”„ ë°€ë„
        else:
            features.append(0.0)
        features.append(graph_edges / graph_nodes)  # í‰ê·  ì—°ê²°ë„
    else:
        features.extend([0.0, 0.0])
    
    # 11. Rule ë‹¤ì–‘ì„± ë° ê°•ë„
    unique_rules = len(set(r.get("rule_id", "") for r in rule_results))
    if len(rule_results) > 0:
        features.append(unique_rules / len(rule_results))  # Rule ë‹¤ì–‘ì„±
    else:
        features.append(0.0)
    
    # ì‹¬ê°ë„ ê°€ì¤‘ í‰ê· 
    severity_scores = {"CRITICAL": 4, "HIGH": 3, "MEDIUM": 2, "LOW": 1}
    if len(severities) > 0:
        avg_severity = np.mean([severity_scores.get(s, 2) for s in severities])
        features.append(avg_severity / 4.0)  # ì •ê·œí™”
        max_severity = max([severity_scores.get(s, 0) for s in severities])
        features.append(max_severity / 4.0)  # ìµœëŒ€ ì‹¬ê°ë„
    else:
        features.extend([0.0, 0.0])
    
    # 12. PPR ìƒí˜¸ì‘ìš©
    ppr_score = ml_features.get("ppr_score", 0.0)
    sdn_ppr = ml_features.get("sdn_ppr", 0.0)
    mixer_ppr = ml_features.get("mixer_ppr", 0.0)
    features.append(ppr_score * rule_score / 100.0)  # PPR Ã— Rule
    features.append(sdn_ppr + mixer_ppr)  # ì´ ìœ„í—˜ PPR
    
    # 13. íŒ¨í„´ ì ìˆ˜ ìƒí˜¸ì‘ìš©
    pattern_score = ml_features.get("pattern_score", 0.0)
    features.append(pattern_score * graph_score / 100.0)  # Pattern Ã— Graph
    features.append(pattern_score / (rule_score + 1.0))  # Pattern / Rule ë¹„ìœ¨
    
    # 14. ì •ê·œí™” ì ìˆ˜ ìƒí˜¸ì‘ìš©
    n_theta = ml_features.get("n_theta", 0.0)
    n_omega = ml_features.get("n_omega", 0.0)
    features.append(n_theta * n_omega)  # NTS Ã— NWS
    features.append(abs(n_theta - n_omega))  # NTS-NWS ì°¨ì´
    
    # NaN, Inf ì²´í¬
    features_array = np.array(features, dtype=np.float32)
    features_array = np.nan_to_num(features_array, nan=0.0, posinf=100.0, neginf=0.0)
    
    return features_array


def load_full_dataset(file_path: Path) -> Tuple[List[np.ndarray], List[int]]:
    """ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ ë° feature ì¶”ì¶œ"""
    print(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ: {file_path.name}")
    with open(file_path, 'r') as f:
        data = json.load(f)
    
    stage1_scorer = Stage1Scorer(rule_weight=0.9, graph_weight=0.1)
    
    features = []
    labels = []
    
    print("   Feature ì¶”ì¶œ ì¤‘...")
    for i, item in enumerate(data):
        if (i + 1) % 1000 == 0:
            print(f"      {i + 1}/{len(data)} ì²˜ë¦¬ ì¤‘...")
        
        label = item.get("ground_truth_label", "normal")
        labels.append(1 if label == "fraud" else 0)
        
        tx_data = {
            "from": item.get("from", ""),
            "to": item.get("to", ""),
            "usd_value": item.get("usd_value", 0),
            "timestamp": item.get("timestamp", 0),
            "tx_hash": item.get("tx_hash", ""),
            "chain": item.get("chain", "ethereum"),
            "is_sanctioned": item.get("tx_context", {}).get("is_sanctioned", False),
            "is_mixer": item.get("tx_context", {}).get("is_mixer", False),
        }
        
        ml_features = item.get("ml_features", {})
        tx_context = item.get("tx_context", {})
        
        try:
            stage1_result = stage1_scorer.calculate_risk_score(tx_data, ml_features, tx_context)
            feature_vector = extract_advanced_features(stage1_result, ml_features, tx_context)
            features.append(feature_vector)
        except Exception as e:
            # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ feature ë²¡í„° ì‚¬ìš©
            features.append(np.zeros(55, dtype=np.float32))
    
    print(f"   ì™„ë£Œ: {len(features)}ê°œ ìƒ˜í”Œ, {len(features[0]) if features else 0}ì°¨ì›")
    return features, labels


def optimize_xgboost(X_train, y_train, X_val, y_val, scaler):
    """XGBoost ìµœì í™”"""
    if not XGBOOST_AVAILABLE:
        return None
    
    print("\nğŸ” XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì¤‘...")
    
    X_train_scaled = scaler.transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    
    # ë” ë„“ì€ íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
    param_dist = {
        'n_estimators': randint(200, 500),
        'max_depth': randint(5, 10),
        'learning_rate': uniform(0.01, 0.2),
        'subsample': uniform(0.7, 0.3),
        'colsample_bytree': uniform(0.7, 0.3),
        'min_child_weight': randint(1, 5),
        'gamma': uniform(0, 0.5)
    }
    
    xgb = XGBClassifier(
        random_state=42,
        eval_metric='logloss',
        use_label_encoder=False
    )
    
    # RandomizedSearchCV (ë” ë¹ ë¦„)
    random_search = RandomizedSearchCV(
        xgb,
        param_dist,
        n_iter=50,  # 50ê°œ ì¡°í•© ì‹œë„
        cv=3,
        scoring=make_scorer(f1_score),
        n_jobs=-1,
        random_state=42,
        verbose=1
    )
    
    random_search.fit(X_train_scaled, y_train)
    
    best_xgb = random_search.best_estimator_
    y_pred_proba = best_xgb.predict_proba(X_val_scaled)[:, 1]
    y_pred = best_xgb.predict(X_val_scaled)
    
    metrics = {
        "accuracy": accuracy_score(y_val, y_pred),
        "precision": precision_score(y_val, y_pred, zero_division=0),
        "recall": recall_score(y_val, y_pred, zero_division=0),
        "f1_score": f1_score(y_val, y_pred, zero_division=0),
        "roc_auc": roc_auc_score(y_val, y_pred_proba) if len(set(y_val)) > 1 else 0.5
    }
    
    print(f"   ìµœì  íŒŒë¼ë¯¸í„°: {random_search.best_params_}")
    print(f"   F1-Score: {metrics['f1_score']:.4f}, Accuracy: {metrics['accuracy']:.4f}")
    
    return {
        "model": best_xgb,
        "params": random_search.best_params_,
        "metrics": metrics
    }


def optimize_lightgbm(X_train, y_train, X_val, y_val, scaler):
    """LightGBM ìµœì í™”"""
    if not LIGHTGBM_AVAILABLE:
        return None
    
    print("\nğŸ” LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì¤‘...")
    
    X_train_scaled = scaler.transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    
    param_dist = {
        'n_estimators': randint(200, 500),
        'max_depth': randint(5, 10),
        'learning_rate': uniform(0.01, 0.2),
        'subsample': uniform(0.7, 0.3),
        'colsample_bytree': uniform(0.7, 0.3),
        'min_child_samples': randint(10, 50),
        'reg_alpha': uniform(0, 1),
        'reg_lambda': uniform(0, 1)
    }
    
    lgbm = LGBMClassifier(
        random_state=42,
        verbose=-1
    )
    
    random_search = RandomizedSearchCV(
        lgbm,
        param_dist,
        n_iter=50,
        cv=3,
        scoring=make_scorer(f1_score),
        n_jobs=-1,
        random_state=42,
        verbose=1
    )
    
    random_search.fit(X_train_scaled, y_train)
    
    best_lgbm = random_search.best_estimator_
    y_pred_proba = best_lgbm.predict_proba(X_val_scaled)[:, 1]
    y_pred = best_lgbm.predict(X_val_scaled)
    
    metrics = {
        "accuracy": accuracy_score(y_val, y_pred),
        "precision": precision_score(y_val, y_pred, zero_division=0),
        "recall": recall_score(y_val, y_pred, zero_division=0),
        "f1_score": f1_score(y_val, y_pred, zero_division=0),
        "roc_auc": roc_auc_score(y_val, y_pred_proba) if len(set(y_val)) > 1 else 0.5
    }
    
    print(f"   ìµœì  íŒŒë¼ë¯¸í„°: {random_search.best_params_}")
    print(f"   F1-Score: {metrics['f1_score']:.4f}, Accuracy: {metrics['accuracy']:.4f}")
    
    return {
        "model": best_lgbm,
        "params": random_search.best_params_,
        "metrics": metrics
    }


def optimize_gradient_boosting(X_train, y_train, X_val, y_val, scaler):
    """Gradient Boosting ìµœì í™”"""
    print("\nğŸ” Gradient Boosting í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì¤‘...")
    
    X_train_scaled = scaler.transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    
    param_grid = {
        'n_estimators': [300, 400, 500],
        'max_depth': [5, 7, 9],
        'learning_rate': [0.05, 0.1, 0.15],
        'subsample': [0.8, 0.9, 1.0],
        'min_samples_split': [5, 10, 15],
        'min_samples_leaf': [2, 4, 6]
    }
    
    gb = GradientBoostingClassifier(random_state=42)
    
    grid_search = GridSearchCV(
        gb,
        param_grid,
        cv=3,
        scoring=make_scorer(f1_score),
        n_jobs=-1,
        verbose=1
    )
    
    grid_search.fit(X_train_scaled, y_train)
    
    best_gb = grid_search.best_estimator_
    y_pred_proba = best_gb.predict_proba(X_val_scaled)[:, 1]
    y_pred = best_gb.predict(X_val_scaled)
    
    metrics = {
        "accuracy": accuracy_score(y_val, y_pred),
        "precision": precision_score(y_val, y_pred, zero_division=0),
        "recall": recall_score(y_val, y_pred, zero_division=0),
        "f1_score": f1_score(y_val, y_pred, zero_division=0),
        "roc_auc": roc_auc_score(y_val, y_pred_proba) if len(set(y_val)) > 1 else 0.5
    }
    
    print(f"   ìµœì  íŒŒë¼ë¯¸í„°: {grid_search.best_params_}")
    print(f"   F1-Score: {metrics['f1_score']:.4f}, Accuracy: {metrics['accuracy']:.4f}")
    
    return {
        "model": best_gb,
        "params": grid_search.best_params_,
        "metrics": metrics
    }


def create_advanced_ensemble(models_dict: Dict[str, Any], X_train, y_train, X_val, y_val, scaler):
    """ê³ ê¸‰ ì•™ìƒë¸” ëª¨ë¸ ìƒì„±"""
    print("\nğŸ¤– ê³ ê¸‰ ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ì¤‘...")
    
    X_train_scaled = scaler.transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤ ìˆ˜ì§‘
    estimators = []
    
    if "xgboost" in models_dict and models_dict["xgboost"]:
        estimators.append(('xgb', models_dict["xgboost"]["model"]))
    
    if "lightgbm" in models_dict and models_dict["lightgbm"]:
        estimators.append(('lgbm', models_dict["lightgbm"]["model"]))
    
    if "gradient_boosting" in models_dict and models_dict["gradient_boosting"]:
        estimators.append(('gb', models_dict["gradient_boosting"]["model"]))
    
    if len(estimators) < 2:
        print("   âš ï¸  ì•™ìƒë¸”ì„ ìœ„í•œ ëª¨ë¸ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        return None
    
    # Voting Classifier
    voting_clf = VotingClassifier(
        estimators=estimators,
        voting='soft',
        weights=[2, 2, 1] if len(estimators) == 3 else [2, 1]
    )
    
    voting_clf.fit(X_train_scaled, y_train)
    
    y_pred_proba = voting_clf.predict_proba(X_val_scaled)[:, 1]
    y_pred = voting_clf.predict(X_val_scaled)
    
    metrics = {
        "accuracy": accuracy_score(y_val, y_pred),
        "precision": precision_score(y_val, y_pred, zero_division=0),
        "recall": recall_score(y_val, y_pred, zero_division=0),
        "f1_score": f1_score(y_val, y_pred, zero_division=0),
        "roc_auc": roc_auc_score(y_val, y_pred_proba) if len(set(y_val)) > 1 else 0.5
    }
    
    print(f"   ì•™ìƒë¸” ì„±ëŠ¥:")
    print(f"   Accuracy: {metrics['accuracy']:.4f}, F1: {metrics['f1_score']:.4f}")
    
    return {
        "model": voting_clf,
        "metrics": metrics
    }


def optimize_threshold(y_true, y_pred_proba):
    """Threshold ìµœì í™” (F1-Score ìµœëŒ€í™”)"""
    best_threshold = 0.5
    best_f1 = 0.0
    
    thresholds = np.arange(0.1, 0.9, 0.01)
    
    for threshold in thresholds:
        y_pred = [1 if p >= threshold else 0 for p in y_pred_proba]
        f1 = f1_score(y_true, y_pred, zero_division=0)
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
    
    return best_threshold


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 80)
    print("ì„±ëŠ¥ ìµœëŒ€í™” (ëª©í‘œ: 85% ì´ìƒ)")
    print("=" * 80)
    
    dataset_dir = project_root / "data" / "dataset"
    
    # ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš© ì‹œë„
    full_dataset_path = dataset_dir / "diverse_rules_enhanced.json"
    sampled_dataset_path = dataset_dir / "diverse_rules_enhanced_sampled.json"
    
    if full_dataset_path.exists():
        file_size = full_dataset_path.stat().st_size / (1024 * 1024)  # MB
        print(f"\nâœ… ì „ì²´ ë°ì´í„°ì…‹ ë°œê²¬! (ì•½ {file_size:.1f}MB)")
        print("   âš ï¸  ì „ì²´ ë°ì´í„°ì…‹ì€ ë§¤ìš° í½ë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„°ì…‹ ì‚¬ìš© ê¶Œì¥.")
        print("   ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš©í•˜ë ¤ë©´ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”.")
        # dataset_path = full_dataset_path  # ì£¼ì„ í•´ì œí•˜ì—¬ ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš©
        dataset_path = sampled_dataset_path  # ìƒ˜í”Œ ë°ì´í„°ì…‹ ì‚¬ìš© (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
    elif sampled_dataset_path.exists():
        print("\nâš ï¸  ì „ì²´ ë°ì´í„°ì…‹ ì—†ìŒ. ìƒ˜í”Œ ë°ì´í„°ì…‹ ì‚¬ìš©.")
        dataset_path = sampled_dataset_path
    else:
        # train.json ì‚¬ìš©
        dataset_path = dataset_dir / "train.json"
        if not dataset_path.exists():
            print(f"âŒ ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
    
    if not dataset_path.exists():
        print(f"âŒ ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dataset_path}")
        return
    
    # ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    X_all, y_all = load_full_dataset(dataset_path)
    X_all = np.array(X_all)
    y_all = np.array(y_all)
    
    print(f"\nğŸ“Š ë°ì´í„°ì…‹ í†µê³„:")
    print(f"   ì´ ìƒ˜í”Œ: {len(X_all)}ê°œ")
    print(f"   Feature ì°¨ì›: {X_all.shape[1]}ì°¨ì›")
    print(f"   Fraud: {y_all.sum()}ê°œ ({y_all.sum()/len(y_all)*100:.1f}%)")
    print(f"   Normal: {(len(y_all)-y_all.sum())}ê°œ ({(len(y_all)-y_all.sum())/len(y_all)*100:.1f}%)")
    
    # ë°ì´í„° ë¶„í•  (80:10:10)
    from sklearn.model_selection import train_test_split
    
    X_train, X_temp, y_train, y_temp = train_test_split(
        X_all, y_all, test_size=0.2, random_state=42, stratify=y_all
    )
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
    )
    
    print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
    print(f"   Train: {len(X_train)}ê°œ ({len(X_train)/len(X_all)*100:.1f}%)")
    print(f"   Val: {len(X_val)}ê°œ ({len(X_val)/len(X_all)*100:.1f}%)")
    print(f"   Test: {len(X_test)}ê°œ ({len(X_test)/len(X_all)*100:.1f}%)")
    
    # Feature ìŠ¤ì¼€ì¼ë§
    print("\nğŸ”§ Feature ìŠ¤ì¼€ì¼ë§ ì¤‘...")
    scaler = RobustScaler()  # RobustScaler ì‚¬ìš© (ì´ìƒì¹˜ì— ê°•í•¨)
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)
    
    # ëª¨ë¸ ìµœì í™”
    models = {}
    
    # 1. XGBoost
    if XGBOOST_AVAILABLE:
        xgb_result = optimize_xgboost(X_train, y_train, X_val, y_val, scaler)
        if xgb_result:
            models["xgboost"] = xgb_result
    
    # 2. LightGBM
    if LIGHTGBM_AVAILABLE:
        lgbm_result = optimize_lightgbm(X_train, y_train, X_val, y_val, scaler)
        if lgbm_result:
            models["lightgbm"] = lgbm_result
    
    # 3. Gradient Boosting
    gb_result = optimize_gradient_boosting(X_train, y_train, X_val, y_val, scaler)
    if gb_result:
        models["gradient_boosting"] = gb_result
    
    # 4. ì•™ìƒë¸” ëª¨ë¸
    ensemble_result = create_advanced_ensemble(models, X_train, y_train, X_val, y_val, scaler)
    if ensemble_result:
        models["ensemble"] = ensemble_result
    
    # ìµœê³  ëª¨ë¸ ì„ íƒ
    best_model_name = None
    best_accuracy = 0.0
    
    for name, result in models.items():
        if "metrics" in result:
            acc = result["metrics"]["accuracy"]
            if acc > best_accuracy:
                best_accuracy = acc
                best_model_name = name
    
    if not best_model_name:
        print("\nâŒ ìµœê³  ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nğŸ† ìµœê³  ëª¨ë¸: {best_model_name}")
    best_model = models[best_model_name]["model"]
    best_metrics = models[best_model_name]["metrics"]
    
    print(f"   Validation ì„±ëŠ¥:")
    print(f"   Accuracy: {best_metrics['accuracy']:.4f} ({best_metrics['accuracy']*100:.2f}%)")
    print(f"   Precision: {best_metrics['precision']:.4f}")
    print(f"   Recall: {best_metrics['recall']:.4f}")
    print(f"   F1-Score: {best_metrics['f1_score']:.4f}")
    print(f"   ROC-AUC: {best_metrics['roc_auc']:.4f}")
    
    # Threshold ìµœì í™”
    print("\nğŸ¯ Threshold ìµœì í™” ì¤‘...")
    y_val_proba = best_model.predict_proba(X_val_scaled)[:, 1]
    optimal_threshold = optimize_threshold(y_val, y_val_proba)
    print(f"   ìµœì  Threshold: {optimal_threshold:.3f}")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ìµœì¢… í‰ê°€
    print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ìµœì¢… í‰ê°€...")
    y_test_proba = best_model.predict_proba(X_test_scaled)[:, 1]
    y_test_pred = [1 if p >= optimal_threshold else 0 for p in y_test_proba]
    
    test_accuracy = accuracy_score(y_test, y_test_pred)
    test_precision = precision_score(y_test, y_test_pred, zero_division=0)
    test_recall = recall_score(y_test, y_test_pred, zero_division=0)
    test_f1 = f1_score(y_test, y_test_pred, zero_division=0)
    test_roc_auc = roc_auc_score(y_test, y_test_proba) if len(set(y_test)) > 1 else 0.5
    
    print(f"\nâœ… ìµœì¢… ì„±ëŠ¥ (Test Set):")
    print(f"   Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")
    print(f"   Precision: {test_precision:.4f}")
    print(f"   Recall: {test_recall:.4f}")
    print(f"   F1-Score: {test_f1:.4f}")
    print(f"   ROC-AUC: {test_roc_auc:.4f}")
    print(f"   Threshold: {optimal_threshold:.3f}")
    
    if test_accuracy >= 0.85:
        print(f"\nğŸ‰ ëª©í‘œ ë‹¬ì„±! Accuracy {test_accuracy*100:.2f}% >= 85%")
    else:
        print(f"\nâš ï¸  ëª©í‘œ ë¯¸ë‹¬ì„±. Accuracy {test_accuracy*100:.2f}% < 85%")
        print(f"   ì¶”ê°€ ê°œì„  í•„ìš”: {85 - test_accuracy*100:.2f}%p")
    
    # ëª¨ë¸ ì €ì¥
    output_dir = project_root / "models"
    output_dir.mkdir(exist_ok=True)
    
    with open(output_dir / "maximized_model.pkl", 'wb') as f:
        pickle.dump({
            "model": best_model,
            "scaler": scaler,
            "threshold": optimal_threshold,
            "model_name": best_model_name,
            "feature_dim": X_train.shape[1],
            "test_metrics": {
                "accuracy": test_accuracy,
                "precision": test_precision,
                "recall": test_recall,
                "f1_score": test_f1,
                "roc_auc": test_roc_auc
            }
        }, f)
    
    print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥: {output_dir / 'maximized_model.pkl'}")
    
    # ê²°ê³¼ ì €ì¥
    results = {
        "best_model": best_model_name,
        "best_metrics": best_metrics,
        "test_metrics": {
            "accuracy": test_accuracy,
            "precision": test_precision,
            "recall": test_recall,
            "f1_score": test_f1,
            "roc_auc": test_roc_auc
        },
        "optimal_threshold": optimal_threshold,
        "all_models": {name: result.get("metrics", {}) for name, result in models.items() if "metrics" in result}
    }
    
    with open(output_dir / "maximization_results.json", 'w') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_dir / 'maximization_results.json'}")


if __name__ == "__main__":
    main()

