#!/usr/bin/env python3
"""
Hybrid ëª¨ë¸ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ Hybrid ëª¨ë¸ í‰ê°€ ë° Rule-based, MPOCryptoMLê³¼ ë¹„êµ

ì‚¬ìš©ë²•:
    python scripts/evaluate_hybrid_model.py
"""
import sys
import json
import pickle
from pathlib import Path
from typing import Dict, List, Any, Tuple
import numpy as np
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score, confusion_matrix
)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.scoring.ai_weight_learner import RuleWeightLearner


def extract_hybrid_features(item: Dict[str, Any]) -> np.ndarray:
    """Hybrid ëª¨ë¸ìš© í”¼ì²˜ ì¶”ì¶œ (í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì™€ ë™ì¼)"""
    # 1. Rule-based í”¼ì²˜
    rule_score = item.get("rule_score", 0.0)
    rule_results = item.get("rule_results", [])
    
    rule_axis_scores = {"A": 0.0, "B": 0.0, "C": 0.0, "D": 0.0}
    rule_severity_scores = {"LOW": 0.0, "MEDIUM": 0.0, "HIGH": 0.0, "CRITICAL": 0.0}
    rule_count = len(rule_results)
    
    for rule in rule_results:
        axis = rule.get("axis", "B")
        severity = rule.get("severity", "MEDIUM")
        score = rule.get("score", 0.0)
        
        if axis in rule_axis_scores:
            rule_axis_scores[axis] += score
        if severity in rule_severity_scores:
            rule_severity_scores[severity] += score
    
    # 2. MPOCryptoML í”¼ì²˜
    ml_features = item.get("ml_features", {})
    
    ml_numeric_features = [
        ml_features.get("ppr_score", 0.0),
        ml_features.get("sdn_ppr", 0.0),
        ml_features.get("mixer_ppr", 0.0),
        ml_features.get("pattern_score", 0.0),
        ml_features.get("n_theta", 0.0),
        ml_features.get("n_omega", 0.0),
        ml_features.get("fan_in_count", 0),
        ml_features.get("fan_out_count", 0),
        ml_features.get("gather_scatter", 0.0),
        ml_features.get("graph_nodes", 0),
        ml_features.get("graph_edges", 0),
    ]
    
    detected_patterns = ml_features.get("detected_patterns", [])
    pattern_types = ["fan_in", "fan_out", "gather_scatter", "stack", "bipartite"]
    pattern_features = [1.0 if pattern_type in detected_patterns else 0.0 
                        for pattern_type in pattern_types]
    
    # 3. í†µí•© í”¼ì²˜ ë²¡í„°
    features = [
        rule_score, rule_count,
        rule_axis_scores["A"], rule_axis_scores["B"], rule_axis_scores["C"], rule_axis_scores["D"],
        rule_severity_scores["LOW"], rule_severity_scores["MEDIUM"], 
        rule_severity_scores["HIGH"], rule_severity_scores["CRITICAL"],
        *ml_numeric_features,
        *pattern_features,
    ]
    
    return np.array(features, dtype=np.float32)


def extract_mpocryptml_features(item: Dict[str, Any]) -> np.ndarray:
    """MPOCryptoML ëª¨ë¸ìš© í”¼ì²˜ ì¶”ì¶œ"""
    ml_features = item.get("ml_features", {})
    
    features = [
        ml_features.get("ppr_score", 0.0),
        ml_features.get("sdn_ppr", 0.0),
        ml_features.get("mixer_ppr", 0.0),
        ml_features.get("pattern_score", 0.0),
        ml_features.get("n_theta", 0.0),
        ml_features.get("n_omega", 0.0),
        ml_features.get("fan_in_count", 0),
        ml_features.get("fan_out_count", 0),
        ml_features.get("gather_scatter", 0.0),
        ml_features.get("graph_nodes", 0),
        ml_features.get("graph_edges", 0),
    ]
    
    detected_patterns = ml_features.get("detected_patterns", [])
    pattern_types = ["fan_in", "fan_out", "gather_scatter", "stack", "bipartite"]
    for pattern_type in pattern_types:
        features.append(1.0 if pattern_type in detected_patterns else 0.0)
    
    rule_score = item.get("rule_score", 0.0)
    features.append(rule_score)
    
    return np.array(features, dtype=np.float32)


def evaluate_model_with_predictions(
    y_true: List[int],
    y_pred: List[int],
    y_pred_proba: List[float],
    model_name: str
) -> Dict[str, Any]:
    """ì˜ˆì¸¡ ê²°ê³¼ë¡œë¶€í„° í‰ê°€ ì§€í‘œ ê³„ì‚°"""
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    
    try:
        if len(set(y_pred_proba)) > 1:
            roc_auc = roc_auc_score(y_true, y_pred_proba)
            avg_precision = average_precision_score(y_true, y_pred_proba)
        else:
            roc_auc = 0.5
            avg_precision = 0.0
    except:
        roc_auc = 0.5
        avg_precision = 0.0
    
    cm = confusion_matrix(y_true, y_pred)
    
    return {
        "model_name": model_name,
        "accuracy": float(accuracy),
        "precision": float(precision),
        "recall": float(recall),
        "f1_score": float(f1),
        "roc_auc": float(roc_auc),
        "average_precision": float(avg_precision),
        "confusion_matrix": {
            "true_negative": int(cm[0][0]),
            "false_positive": int(cm[0][1]),
            "false_negative": int(cm[1][0]),
            "true_positive": int(cm[1][1])
        }
    }


def evaluate_hybrid_model(test_data: List[Dict[str, Any]], model_path: Path) -> Dict[str, Any]:
    """Hybrid ëª¨ë¸ í‰ê°€"""
    with open(model_path, 'rb') as f:
        model_data = pickle.load(f)
    
    model = model_data["model"]
    scaler = model_data["scaler"]
    
    X_test = []
    for item in test_data:
        features = extract_hybrid_features(item)
        X_test.append(features)
    
    X_test = np.array(X_test)
    X_test_scaled = scaler.transform(X_test)
    
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    y_pred = model.predict(X_test_scaled)
    
    y_true = []
    for item in test_data:
        label = item.get("ground_truth_label", "normal")
        y_true.append(1 if label == "fraud" else 0)
    
    return evaluate_model_with_predictions(y_true, y_pred, y_pred_proba, "Hybrid (Logistic Regression)")


def evaluate_mpocryptml_model(test_data: List[Dict[str, Any]], model_path: Path) -> Dict[str, Any]:
    """MPOCryptoML ëª¨ë¸ í‰ê°€"""
    with open(model_path, 'rb') as f:
        model_data = pickle.load(f)
    
    model = model_data["model"]
    scaler = model_data["scaler"]
    
    X_test = []
    for item in test_data:
        features = extract_mpocryptml_features(item)
        X_test.append(features)
    
    X_test = np.array(X_test)
    X_test_scaled = scaler.transform(X_test)
    
    y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
    y_pred = model.predict(X_test_scaled)
    
    y_true = []
    for item in test_data:
        label = item.get("ground_truth_label", "normal")
        y_true.append(1 if label == "fraud" else 0)
    
    return evaluate_model_with_predictions(y_true, y_pred, y_pred_proba, "MPOCryptoML (Logistic Regression)")


def evaluate_rule_based(test_data: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Rule-based ëª¨ë¸ í‰ê°€"""
    y_true = []
    y_pred_scores = []
    
    for item in test_data:
        label = item.get("ground_truth_label", "normal")
        y_true.append(1 if label == "fraud" else 0)
        
        # Rule-based ì ìˆ˜
        rule_score = item.get("rule_score", 0.0)
        y_pred_scores.append(rule_score / 100.0)
    
    # Threshold 50.0ìœ¼ë¡œ ë¶„ë¥˜
    y_pred = [1 if score >= 0.5 else 0 for score in y_pred_scores]
    
    return evaluate_model_with_predictions(y_true, y_pred, y_pred_scores, "Rule-based")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("Hybrid ëª¨ë¸ í‰ê°€ ë° ë¹„êµ")
    print("=" * 60)
    
    dataset_dir = project_root / "data" / "dataset"
    test_path = dataset_dir / "test.json"
    hybrid_model_path = project_root / "models" / "hybrid_model_logistic.pkl"
    mpocryptml_model_path = project_root / "models" / "mpocryptml_model.pkl"
    
    if not test_path.exists():
        print(f"âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_path}")
        return
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
    with open(test_path, 'r') as f:
        test_data = json.load(f)
    
    print(f"   ì´ {len(test_data)}ê°œ ìƒ˜í”Œ")
    
    from collections import Counter
    labels = [item.get("ground_truth_label", "normal") for item in test_data]
    label_counts = Counter(labels)
    print(f"   ë¼ë²¨ ë¶„í¬: {dict(label_counts)}")
    
    # ëª¨ë¸ í‰ê°€
    print("\n" + "=" * 60)
    print("ğŸ“Š ëª¨ë¸ í‰ê°€ ì¤‘...")
    print("=" * 60)
    
    all_results = []
    
    # 1. Rule-based
    print("\n1ï¸âƒ£  Rule-based ëª¨ë¸ í‰ê°€ ì¤‘...")
    rule_results = evaluate_rule_based(test_data)
    all_results.append(rule_results)
    
    # 2. MPOCryptoML
    if mpocryptml_model_path.exists():
        print("\n2ï¸âƒ£  MPOCryptoML ëª¨ë¸ í‰ê°€ ì¤‘...")
        mpocryptml_results = evaluate_mpocryptml_model(test_data, mpocryptml_model_path)
        all_results.append(mpocryptml_results)
    else:
        print("\nâš ï¸  MPOCryptoML ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
    
    # 3. Hybrid
    if hybrid_model_path.exists():
        print("\n3ï¸âƒ£  Hybrid ëª¨ë¸ í‰ê°€ ì¤‘...")
        hybrid_results = evaluate_hybrid_model(test_data, hybrid_model_path)
        all_results.append(hybrid_results)
    else:
        print("\nâš ï¸  Hybrid ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“ˆ í‰ê°€ ê²°ê³¼ ë¹„êµ")
    print("=" * 60)
    
    print(f"\n{'ëª¨ë¸':<35} {'Accuracy':<10} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'ROC-AUC':<10}")
    print("-" * 85)
    
    for results in all_results:
        name = results["model_name"]
        acc = results["accuracy"]
        prec = results["precision"]
        rec = results["recall"]
        f1 = results["f1_score"]
        auc = results["roc_auc"]
        
        print(f"{name:<35} {acc:<10.4f} {prec:<10.4f} {rec:<10.4f} {f1:<10.4f} {auc:<10.4f}")
    
    # ìƒì„¸ ê²°ê³¼
    print("\n" + "=" * 60)
    print("ğŸ“‹ ìƒì„¸ ê²°ê³¼")
    print("=" * 60)
    
    for results in all_results:
        print(f"\nğŸ”¹ {results['model_name']}:")
        print(f"   Accuracy:        {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)")
        print(f"   Precision:       {results['precision']:.4f}")
        print(f"   Recall:          {results['recall']:.4f}")
        print(f"   F1-Score:        {results['f1_score']:.4f}")
        print(f"   ROC-AUC:         {results['roc_auc']:.4f}")
        print(f"   Average Precision: {results['average_precision']:.4f}")
        cm = results['confusion_matrix']
        print(f"   Confusion Matrix:")
        print(f"      TN: {cm['true_negative']}, FP: {cm['false_positive']}")
        print(f"      FN: {cm['false_negative']}, TP: {cm['true_positive']}")
    
    # ê²°ê³¼ ì €ì¥
    output_path = dataset_dir / "hybrid_evaluation_results.json"
    with open(output_path, 'w') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥: {output_path}")
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
    if all_results:
        best_model = max(all_results, key=lambda x: x['f1_score'])
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ (F1-Score ê¸°ì¤€): {best_model['model_name']}")
        print(f"   F1-Score: {best_model['f1_score']:.4f}")
        print(f"   Accuracy: {best_model['accuracy']:.4f}")


if __name__ == "__main__":
    main()

