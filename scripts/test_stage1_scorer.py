#!/usr/bin/env python3
"""
1ë‹¨ê³„ ìŠ¤ì½”ì–´ëŸ¬ í…ŒìŠ¤íŠ¸ ë° í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
"""
import sys
import json
from pathlib import Path
from typing import Dict, List, Any
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score, confusion_matrix
)

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.scoring.stage1_scorer import Stage1Scorer


def evaluate_stage1_scorer(
    test_data: List[Dict[str, Any]],
    threshold: float = 50.0
) -> Dict[str, Any]:
    """
    1ë‹¨ê³„ ìŠ¤ì½”ì–´ëŸ¬ í‰ê°€
    
    Args:
        test_data: í…ŒìŠ¤íŠ¸ ë°ì´í„°
        threshold: Risk Score ì„ê³„ê°’ (ê¸°ë³¸ 50.0)
    
    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    scorer = Stage1Scorer()
    
    y_true = []
    y_pred = []
    y_pred_scores = []
    
    rule_scores = []
    graph_scores = []
    
    print("=" * 80)
    print("1ë‹¨ê³„ ìŠ¤ì½”ì–´ëŸ¬ í‰ê°€ ì¤‘...")
    print("=" * 80)
    
    for i, sample in enumerate(test_data):
        if (i + 1) % 100 == 0:
            print(f"  ì²˜ë¦¬ ì¤‘... {i + 1}/{len(test_data)}")
        
        # Ground truth
        label = sample.get("ground_truth_label", "normal")
        y_true.append(1 if label == "fraud" else 0)
        
        # ê±°ë˜ ë°ì´í„° ì¤€ë¹„
        tx_data = {
            "from": sample.get("from", ""),
            "to": sample.get("to", ""),
            "usd_value": sample.get("usd_value", 0),
            "timestamp": sample.get("timestamp", 0),
            "tx_hash": sample.get("tx_hash", ""),
            "chain": sample.get("chain", "ethereum"),
            "is_sanctioned": sample.get("tx_context", {}).get("is_sanctioned", False),
            "is_mixer": sample.get("tx_context", {}).get("is_mixer", False),
        }
        
        # ML features
        ml_features = sample.get("ml_features", {})
        
        # TX context
        tx_context = sample.get("tx_context", {})
        
        # ì ìˆ˜ ê³„ì‚°
        try:
            result = scorer.calculate_risk_score(tx_data, ml_features, tx_context)
            risk_score = result["risk_score"]
            rule_score = result["rule_score"]
            graph_score = result["graph_score"]
            
            y_pred_scores.append(risk_score)
            rule_scores.append(rule_score)
            graph_scores.append(graph_score)
            
            # ì˜ˆì¸¡ ë¼ë²¨
            predicted_label = "fraud" if risk_score >= threshold else "normal"
            y_pred.append(1 if predicted_label == "fraud" else 0)
        except Exception as e:
            print(f"  âš ï¸  ì—ëŸ¬ (ìƒ˜í”Œ {i}): {e}")
            y_pred_scores.append(0.0)
            y_pred.append(0)
            rule_scores.append(0.0)
            graph_scores.append(0.0)
    
    # ì§€í‘œ ê³„ì‚°
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    
    # ROC-AUC ë° Average Precision
    y_pred_proba = [s / 100.0 for s in y_pred_scores]
    roc_auc = 0.5
    avg_precision = 0.0
    if len(set(y_true)) > 1 and len(set(y_pred_proba)) > 1:
        try:
            roc_auc = roc_auc_score(y_true, y_pred_proba)
            avg_precision = average_precision_score(y_true, y_pred_proba)
        except ValueError:
            pass
    
    cm = confusion_matrix(y_true, y_pred)
    
    # í†µê³„
    avg_rule_score = sum(rule_scores) / len(rule_scores) if rule_scores else 0.0
    avg_graph_score = sum(graph_scores) / len(graph_scores) if graph_scores else 0.0
    avg_risk_score = sum(y_pred_scores) / len(y_pred_scores) if y_pred_scores else 0.0
    
    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1_score": f1,
        "roc_auc": roc_auc,
        "average_precision": avg_precision,
        "confusion_matrix": {
            "true_negative": int(cm[0][0]),
            "false_positive": int(cm[0][1]),
            "false_negative": int(cm[1][0]),
            "true_positive": int(cm[1][1])
        },
        "statistics": {
            "avg_rule_score": avg_rule_score,
            "avg_graph_score": avg_graph_score,
            "avg_risk_score": avg_risk_score,
            "threshold": threshold
        }
    }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    dataset_dir = project_root / "data" / "dataset"
    test_path = dataset_dir / "test.json"
    
    if not test_path.exists():
        print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print(f"   ê²½ë¡œ: {test_path}")
        return
    
    # ë°ì´í„° ë¡œë“œ
    print("ğŸ“‚ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì¤‘...")
    with open(test_path, 'r') as f:
        test_data = json.load(f)
    
    print(f"   ì´ {len(test_data)}ê°œ ìƒ˜í”Œ")
    
    # ë¼ë²¨ ë¶„í¬ í™•ì¸
    labels = [item.get("ground_truth_label", "normal") for item in test_data]
    label_counts = {}
    for label in labels:
        label_counts[label] = label_counts.get(label, 0) + 1
    
    print("\në¼ë²¨ ë¶„í¬:")
    for label, count in label_counts.items():
        print(f"  {label}: {count}ê°œ ({count/len(test_data)*100:.1f}%)")
    
    # í‰ê°€
    results = evaluate_stage1_scorer(test_data, threshold=50.0)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 80)
    print("âœ… í‰ê°€ ê²°ê³¼")
    print("=" * 80)
    print(f"\nğŸ“Š ì„±ëŠ¥ ì§€í‘œ:")
    print(f"   Accuracy:  {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)")
    print(f"   Precision: {results['precision']:.4f}")
    print(f"   Recall:    {results['recall']:.4f}")
    print(f"   F1-Score:  {results['f1_score']:.4f}")
    print(f"   ROC-AUC:   {results['roc_auc']:.4f}")
    print(f"   Avg Precision: {results['average_precision']:.4f}")
    
    print(f"\nğŸ“ˆ í†µê³„:")
    stats = results['statistics']
    print(f"   í‰ê·  Rule Score: {stats['avg_rule_score']:.2f}")
    print(f"   í‰ê·  Graph Score: {stats['avg_graph_score']:.2f}")
    print(f"   í‰ê·  Risk Score: {stats['avg_risk_score']:.2f}")
    print(f"   Threshold: {stats['threshold']:.1f}")
    
    print(f"\nğŸ“‹ Confusion Matrix:")
    cm = results['confusion_matrix']
    print(f"   TN: {cm['true_negative']}, FP: {cm['false_positive']}")
    print(f"   FN: {cm['false_negative']}, TP: {cm['true_positive']}")
    
    # ê²°ê³¼ ì €ì¥
    output_path = dataset_dir / "stage1_scorer_results.json"
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")


if __name__ == "__main__":
    main()

