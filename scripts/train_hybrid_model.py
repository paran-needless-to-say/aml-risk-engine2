#!/usr/bin/env python3
"""
Hybrid ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

Rule-based ì ìˆ˜ + MPOCryptoML í”¼ì²˜ë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… ë¦¬ìŠ¤í¬ ì ìˆ˜ ì˜ˆì¸¡

ì‚¬ìš©ë²•:
    python scripts/train_hybrid_model.py
"""
import sys
import json
import pickle
from pathlib import Path
from typing import Dict, List, Any, Tuple
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score, confusion_matrix
)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


def extract_hybrid_features(item: Dict[str, Any]) -> np.ndarray:
    """
    Hybrid ëª¨ë¸ìš© í”¼ì²˜ ì¶”ì¶œ
    
    Rule-based ì ìˆ˜ + MPOCryptoML í”¼ì²˜ë¥¼ ëª¨ë‘ í¬í•¨
    
    Args:
        item: ë°ì´í„°ì…‹ í•­ëª©
    
    Returns:
        í”¼ì²˜ ë²¡í„° (numpy array)
    """
    # 1. Rule-based í”¼ì²˜
    rule_score = item.get("rule_score", 0.0)
    rule_results = item.get("rule_results", [])
    
    # Rule-based ì„¸ë¶€ í”¼ì²˜
    rule_axis_scores = {"A": 0.0, "B": 0.0, "C": 0.0, "D": 0.0}
    rule_severity_scores = {"LOW": 0.0, "MEDIUM": 0.0, "HIGH": 0.0, "CRITICAL": 0.0}
    rule_count = len(rule_results)
    
    for rule in rule_results:
        axis = rule.get("axis", "B")
        severity = rule.get("severity", "MEDIUM")
        score = rule.get("score", 0.0)
        
        if axis in rule_axis_scores:
            rule_axis_scores[axis] += score
        if severity in rule_severity_scores:
            rule_severity_scores[severity] += score
    
    # 2. MPOCryptoML í”¼ì²˜
    ml_features = item.get("ml_features", {})
    
    ml_numeric_features = [
        ml_features.get("ppr_score", 0.0),
        ml_features.get("sdn_ppr", 0.0),
        ml_features.get("mixer_ppr", 0.0),
        ml_features.get("pattern_score", 0.0),
        ml_features.get("n_theta", 0.0),
        ml_features.get("n_omega", 0.0),
        ml_features.get("fan_in_count", 0),
        ml_features.get("fan_out_count", 0),
        ml_features.get("gather_scatter", 0.0),
        ml_features.get("graph_nodes", 0),
        ml_features.get("graph_edges", 0),
    ]
    
    # íŒ¨í„´ í”¼ì²˜ (one-hot encoding)
    detected_patterns = ml_features.get("detected_patterns", [])
    pattern_types = ["fan_in", "fan_out", "gather_scatter", "stack", "bipartite"]
    pattern_features = [1.0 if pattern_type in detected_patterns else 0.0 
                        for pattern_type in pattern_types]
    
    # 3. í†µí•© í”¼ì²˜ ë²¡í„°
    features = [
        # Rule-based ê¸°ë³¸
        rule_score,
        rule_count,
        # Rule-based Axisë³„ ì ìˆ˜
        rule_axis_scores["A"],
        rule_axis_scores["B"],
        rule_axis_scores["C"],
        rule_axis_scores["D"],
        # Rule-based Severityë³„ ì ìˆ˜
        rule_severity_scores["LOW"],
        rule_severity_scores["MEDIUM"],
        rule_severity_scores["HIGH"],
        rule_severity_scores["CRITICAL"],
        # MPOCryptoML í”¼ì²˜
        *ml_numeric_features,
        *pattern_features,
    ]
    
    return np.array(features, dtype=np.float32)


def load_dataset(file_path: Path) -> Tuple[List[np.ndarray], List[int]]:
    """
    ë°ì´í„°ì…‹ ë¡œë“œ ë° í”¼ì²˜/ë¼ë²¨ ì¶”ì¶œ
    
    Args:
        file_path: ë°ì´í„°ì…‹ JSON íŒŒì¼ ê²½ë¡œ
    
    Returns:
        (features, labels) íŠœí”Œ
    """
    with open(file_path, 'r') as f:
        data = json.load(f)
    
    features = []
    labels = []
    
    for item in data:
        # í”¼ì²˜ ì¶”ì¶œ
        feature_vector = extract_hybrid_features(item)
        features.append(feature_vector)
        
        # ë¼ë²¨ ë³€í™˜ (fraud=1, normal=0)
        label = item.get("ground_truth_label", "normal")
        labels.append(1 if label == "fraud" else 0)
    
    return features, labels


def train_hybrid_model(
    train_path: Path,
    val_path: Path,
    output_path: Path,
    model_type: str = "logistic"
) -> Dict[str, Any]:
    """
    Hybrid ëª¨ë¸ í•™ìŠµ
    
    Args:
        train_path: í•™ìŠµ ë°ì´í„°ì…‹ ê²½ë¡œ
        val_path: ê²€ì¦ ë°ì´í„°ì…‹ ê²½ë¡œ
        output_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        model_type: ëª¨ë¸ íƒ€ì… ("logistic", "gradient_boosting", "random_forest")
    
    Returns:
        í•™ìŠµ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    print("=" * 60)
    print("Hybrid ëª¨ë¸ í•™ìŠµ (Rule-based + MPOCryptoML)")
    print("=" * 60)
    
    # ë°ì´í„° ë¡œë“œ
    print("\nğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    X_train, y_train = load_dataset(train_path)
    X_val, y_val = load_dataset(val_path)
    
    X_train = np.array(X_train)
    X_val = np.array(X_val)
    y_train = np.array(y_train)
    y_val = np.array(y_val)
    
    print(f"   í•™ìŠµ ë°ì´í„°: {len(X_train)}ê°œ")
    print(f"   ê²€ì¦ ë°ì´í„°: {len(X_val)}ê°œ")
    print(f"   í”¼ì²˜ ì°¨ì›: {X_train.shape[1]}ê°œ")
    
    # í”¼ì²˜ ìŠ¤ì¼€ì¼ë§
    print("\nğŸ”§ í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ ì¤‘...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    
    # ëª¨ë¸ ì„ íƒ ë° í•™ìŠµ
    print(f"\nğŸ¯ ëª¨ë¸ í•™ìŠµ ì¤‘ ({model_type})...")
    
    if model_type == "logistic":
        model = LogisticRegression(
            max_iter=1000,
            random_state=42,
            class_weight='balanced'
        )
    elif model_type == "gradient_boosting":
        model = GradientBoostingClassifier(
            n_estimators=100,
            max_depth=5,
            random_state=42
        )
    elif model_type == "random_forest":
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=5,
            random_state=42,
            class_weight='balanced'
        )
    else:
        raise ValueError(f"Unknown model type: {model_type}")
    
    model.fit(X_train_scaled, y_train)
    
    # ê²€ì¦ ë°ì´í„° ì˜ˆì¸¡
    y_pred = model.predict(X_val_scaled)
    y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]
    
    # í‰ê°€ ì§€í‘œ ê³„ì‚°
    accuracy = accuracy_score(y_val, y_pred)
    precision = precision_score(y_val, y_pred, zero_division=0)
    recall = recall_score(y_val, y_pred, zero_division=0)
    f1 = f1_score(y_val, y_pred, zero_division=0)
    
    try:
        roc_auc = roc_auc_score(y_val, y_pred_proba) if len(set(y_pred_proba)) > 1 else 0.5
        avg_precision = average_precision_score(y_val, y_pred_proba)
    except:
        roc_auc = 0.5
        avg_precision = 0.0
    
    cm = confusion_matrix(y_val, y_pred)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    print("=" * 60)
    print(f"\nğŸ“Š ê²€ì¦ ì„±ëŠ¥:")
    print(f"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"   Precision: {precision:.4f}")
    print(f"   Recall:    {recall:.4f}")
    print(f"   F1-Score:  {f1:.4f}")
    print(f"   ROC-AUC:   {roc_auc:.4f}")
    print(f"   Avg Precision: {avg_precision:.4f}")
    
    print(f"\nğŸ“ˆ Confusion Matrix:")
    print(f"   True Negative:  {cm[0][0]}")
    print(f"   False Positive: {cm[0][1]}")
    print(f"   False Negative: {cm[1][0]}")
    print(f"   True Positive:  {cm[1][1]}")
    
    # ëª¨ë¸ ì €ì¥
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    model_data = {
        "model": model,
        "scaler": scaler,
        "model_type": model_type,
        "feature_names": [
            "rule_score", "rule_count",
            "rule_axis_A", "rule_axis_B", "rule_axis_C", "rule_axis_D",
            "rule_severity_LOW", "rule_severity_MEDIUM", "rule_severity_HIGH", "rule_severity_CRITICAL",
            "ppr_score", "sdn_ppr", "mixer_ppr", "pattern_score",
            "n_theta", "n_omega", "fan_in_count", "fan_out_count",
            "gather_scatter", "graph_nodes", "graph_edges",
            "pattern_fan_in", "pattern_fan_out", "pattern_gather_scatter",
            "pattern_stack", "pattern_bipartite"
        ]
    }
    
    with open(output_path, 'wb') as f:
        pickle.dump(model_data, f)
    
    print(f"\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_path}")
    
    # í•™ìŠµ ê²°ê³¼ ë°˜í™˜
    results = {
        "model_type": model_type,
        "accuracy": float(accuracy),
        "precision": float(precision),
        "recall": float(recall),
        "f1_score": float(f1),
        "roc_auc": float(roc_auc),
        "average_precision": float(avg_precision),
        "confusion_matrix": {
            "true_negative": int(cm[0][0]),
            "false_positive": int(cm[0][1]),
            "false_negative": int(cm[1][0]),
            "true_positive": int(cm[1][1])
        }
    }
    
    return results


def compare_models(
    train_path: Path,
    val_path: Path,
    output_dir: Path
) -> Dict[str, Any]:
    """
    ì—¬ëŸ¬ ëª¨ë¸ íƒ€ì… ë¹„êµ í•™ìŠµ
    
    Args:
        train_path: í•™ìŠµ ë°ì´í„°ì…‹ ê²½ë¡œ
        val_path: ê²€ì¦ ë°ì´í„°ì…‹ ê²½ë¡œ
        output_dir: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
    
    Returns:
        ëª¨ë“  ëª¨ë¸ì˜ í•™ìŠµ ê²°ê³¼
    """
    model_types = ["logistic", "gradient_boosting", "random_forest"]
    all_results = {}
    
    for model_type in model_types:
        print(f"\n{'='*60}")
        print(f"ëª¨ë¸ íƒ€ì…: {model_type}")
        print(f"{'='*60}")
        
        output_path = output_dir / f"hybrid_model_{model_type}.pkl"
        results = train_hybrid_model(train_path, val_path, output_path, model_type)
        all_results[model_type] = results
    
    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
    best_model = max(all_results.items(), key=lambda x: x[1]['f1_score'])
    print(f"\n{'='*60}")
    print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model[0]}")
    print(f"   F1-Score: {best_model[1]['f1_score']:.4f}")
    print(f"   Accuracy: {best_model[1]['accuracy']:.4f}")
    print(f"{'='*60}")
    
    return all_results


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Hybrid ëª¨ë¸ í•™ìŠµ")
    parser.add_argument(
        "--model-type",
        type=str,
        default="all",
        choices=["logistic", "gradient_boosting", "random_forest", "all"],
        help="í•™ìŠµí•  ëª¨ë¸ íƒ€ì… (ê¸°ë³¸: all)"
    )
    args = parser.parse_args()
    
    dataset_dir = project_root / "data" / "dataset"
    train_path = dataset_dir / "train.json"
    val_path = dataset_dir / "val.json"
    models_dir = project_root / "models"
    
    if not train_path.exists():
        print(f"âŒ í•™ìŠµ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_path}")
        return
    
    if not val_path.exists():
        print(f"âŒ ê²€ì¦ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {val_path}")
        return
    
    models_dir.mkdir(parents=True, exist_ok=True)
    
    if args.model_type == "all":
        # ëª¨ë“  ëª¨ë¸ ë¹„êµ í•™ìŠµ
        all_results = compare_models(train_path, val_path, models_dir)
        
        # ê²°ê³¼ ì €ì¥
        results_path = dataset_dir / "hybrid_training_results.json"
        with open(results_path, 'w') as f:
            json.dump(all_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ í•™ìŠµ ê²°ê³¼ ì €ì¥: {results_path}")
    else:
        # ë‹¨ì¼ ëª¨ë¸ í•™ìŠµ
        output_path = models_dir / f"hybrid_model_{args.model_type}.pkl"
        results = train_hybrid_model(train_path, val_path, output_path, args.model_type)
        
        # ê²°ê³¼ ì €ì¥
        results_path = dataset_dir / f"hybrid_training_results_{args.model_type}.json"
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ í•™ìŠµ ê²°ê³¼ ì €ì¥: {results_path}")
    
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("1. ëª¨ë¸ í‰ê°€: python scripts/evaluate_hybrid_model.py")
    print("2. Rule-based, MPOCryptoML, Hybrid ëª¨ë¸ ë¹„êµ")


if __name__ == "__main__":
    main()

