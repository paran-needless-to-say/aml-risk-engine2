#!/usr/bin/env python3
"""
ë‹¤ì–‘í•œ ë£°ì´ ë°œë™ë˜ë„ë¡ ë°ì´í„° ìˆ˜ì§‘ ë° ê°œì„ 

í˜„ì¬ ë¬¸ì œ: ëª¨ë“  ìƒ˜í”Œì´ B-501ë§Œ ë°œë™
í•´ê²°: ë” ë§ì€ ë£°ì´ ë°œë™ë˜ë„ë¡ ë°ì´í„° ìˆ˜ì§‘ ë° ë£° í‰ê°€ ê°œì„ 
"""
import sys
import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Set
from tqdm import tqdm
from collections import Counter
import networkx as nx
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from core.scoring.dataset_builder import DatasetBuilder
from core.data.lists import ListLoader
from core.data.etherscan_client import EtherscanClient


def enhance_transaction_data(
    tx: Dict[str, Any],
    sdn_addresses: Set[str],
    mixer_addresses: Set[str],
    etherscan_client: EtherscanClient = None
) -> Dict[str, Any]:
    """
    ê±°ë˜ ë°ì´í„°ë¥¼ í–¥ìƒì‹œì¼œ ë” ë§ì€ ë£°ì´ ë°œë™ë˜ë„ë¡ í•¨
    
    - SDN/Mixer í”Œë˜ê·¸ ì¶”ê°€
    - USD ê°’ ì¶”ì • (Wei -> ETH -> USD ëŒ€ëµì  ì¶”ì •)
    - ì£¼ì†Œ íƒœê·¸ ì •ë³´ ì¶”ê°€
    """
    from_addr = tx.get("from", "").lower()
    to_addr = tx.get("to", "").lower()
    
    # SDN/Mixer í”Œë˜ê·¸ ì¶”ê°€
    tx["is_sanctioned"] = (from_addr in sdn_addresses or to_addr in sdn_addresses)
    tx["is_mixer"] = (from_addr in mixer_addresses or to_addr in mixer_addresses)
    
    # USD ê°’ ì¶”ì • (Wei -> ETH -> USD ëŒ€ëµì  ì¶”ì •)
    # ETH ê°€ê²©ì„ 2000 USDë¡œ ê°€ì • (ëŒ€ëµì )
    if tx.get("usd_value", 0) == 0:
        value_wei = tx.get("value", 0)
        if value_wei > 0:
            # Wei -> ETH ë³€í™˜
            value_eth = value_wei / 1e18
            # ETH -> USD ëŒ€ëµì  ì¶”ì • (2000 USD/ETH ê°€ì •)
            estimated_usd = value_eth * 2000.0
            tx["usd_value"] = estimated_usd
            tx["amount_usd"] = estimated_usd
    
    # ì£¼ì†Œ íƒœê·¸ ì •ë³´ ì¶”ê°€ (Etherscan API ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
    if etherscan_client:
        try:
            from_tags = etherscan_client.get_address_tags(from_addr)
            to_tags = etherscan_client.get_address_tags(to_addr)
            
            # íƒœê·¸ ì •ë³´ë¥¼ í”Œë˜ê·¸ë¡œ ë³€í™˜
            if from_tags.get("is_exchange") or to_tags.get("is_exchange"):
                tx["is_cex"] = True
            if from_tags.get("is_bridge") or to_tags.get("is_bridge"):
                tx["is_bridge"] = True
        except:
            pass
    
    return tx


def collect_diverse_rules_data(
    features_path: str,
    transactions_dir: str,
    output_path: str,
    max_transactions_per_contract: int = 100,
    sample_ratio: float = 1.0,
    use_etherscan: bool = False
) -> List[Dict[str, Any]]:
    """
    ë‹¤ì–‘í•œ ë£°ì´ ë°œë™ë˜ë„ë¡ ë°ì´í„° ìˆ˜ì§‘
    
    Args:
        features_path: features CSV íŒŒì¼ ê²½ë¡œ
        transactions_dir: ê±°ë˜ ë°ì´í„° ë””ë ‰í† ë¦¬
        output_path: ì¶œë ¥ JSON íŒŒì¼ ê²½ë¡œ
        max_transactions_per_contract: ì£¼ì†Œë‹¹ ìµœëŒ€ ê±°ë˜ ìˆ˜
        sample_ratio: ìƒ˜í”Œë§ ë¹„ìœ¨
        use_etherscan: Etherscan API ì‚¬ìš© ì—¬ë¶€ (Rate limit ì£¼ì˜)
    """
    print("=" * 80)
    print("ë‹¤ì–‘í•œ ë£° ë°œë™ì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘")
    print("=" * 80)
    
    # Features ë¡œë“œ
    print(f"\nğŸ“‚ Features íŒŒì¼ ë¡œë“œ: {features_path}")
    df = pd.read_csv(features_path)
    df_eth = df[df['Chain'].str.lower() == 'ethereum'].copy()
    
    if sample_ratio < 1.0:
        df_eth = df_eth.sample(frac=sample_ratio, random_state=42)
    
    print(f"   ì²˜ë¦¬í•  ì£¼ì†Œ: {len(df_eth)}ê°œ")
    
    # ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
    list_loader = ListLoader()
    sdn_addresses = list_loader.get_sdn_list()
    mixer_addresses = list_loader.get_mixer_list()
    
    print(f"\nğŸ“‹ ë¦¬ìŠ¤íŠ¸:")
    print(f"   SDN: {len(sdn_addresses)}ê°œ")
    print(f"   Mixer: {len(mixer_addresses)}ê°œ")
    
    # Etherscan í´ë¼ì´ì–¸íŠ¸ (ì„ íƒì )
    etherscan_client = None
    if use_etherscan:
        print("\nâš ï¸  Etherscan API ì‚¬ìš© (Rate limit ì£¼ì˜)")
        etherscan_client = EtherscanClient()
    
    # ë°ì´í„°ì…‹ êµ¬ì¶•ê¸°
    builder = DatasetBuilder()
    
    dataset = []
    transactions_dir_path = Path(transactions_dir)
    
    rule_counter = Counter()
    enhanced_count = 0
    
    print(f"\nğŸ”„ ë°ì´í„° ìˆ˜ì§‘ ë° í–¥ìƒ ì¤‘...")
    
    for idx, row in tqdm(df_eth.iterrows(), total=len(df_eth), desc="ì£¼ì†Œ ì²˜ë¦¬"):
        chain = row['Chain'].lower()
        contract = row['Contract']
        label = int(row.get('label', 0))
        
        tx_file = transactions_dir_path / chain / f"{contract}.csv"
        if not tx_file.exists():
            continue
        
        try:
            df_tx = pd.read_csv(tx_file)
            if max_transactions_per_contract and len(df_tx) > max_transactions_per_contract:
                df_tx = df_tx.sample(n=max_transactions_per_contract, random_state=42)
            
            transactions = []
            for _, tx_row in df_tx.iterrows():
                tx = {
                    "tx_hash": str(tx_row.get("transaction_hash", "")),
                    "from": str(tx_row.get("from", "")),
                    "to": str(tx_row.get("to", "")),
                    "timestamp": int(tx_row.get("timestamp", 0)) if pd.notna(tx_row.get("timestamp")) else 0,
                    "value": int(tx_row.get("value", 0)) if pd.notna(tx_row.get("value")) else 0,
                    "usd_value": 0.0,
                    "chain": chain,
                    "asset_contract": contract,
                    "block_height": int(tx_row.get("block_number", 0)) if pd.notna(tx_row.get("block_number")) else 0,
                }
                
                # ë°ì´í„° í–¥ìƒ (SDN/Mixer í”Œë˜ê·¸, USD ê°’ ì¶”ì •)
                tx = enhance_transaction_data(tx, sdn_addresses, mixer_addresses, etherscan_client)
                transactions.append(tx)
            
            if not transactions:
                continue
            
            # MPOCryptoML í”¼ì²˜ ì¶”ì¶œì„ ìœ„í•œ ê·¸ë˜í”„ êµ¬ì¶•
            graph = nx.DiGraph()
            for tx in transactions:
                from_addr = tx.get("from", "").lower()
                to_addr = tx.get("to", "").lower()
                weight = tx.get("usd_value", tx.get("value", 0) / 1e18)  # USD ì—†ìœ¼ë©´ ETHë¡œ
                if from_addr and to_addr and weight > 0:
                    graph.add_edge(from_addr, to_addr, weight=weight, timestamp=tx.get("timestamp", 0))
            
            # MPOCryptoML í”¼ì²˜ ê³„ì‚°
            ml_features = {}
            if graph.nodes:
                from core.aggregation.mpocryptml_patterns import MPOCryptoMLPatternDetector
                from core.aggregation.mpocryptml_normalizer import MPOCryptoMLNormalizer
                from core.aggregation.ppr_connector import PPRConnector
                
                ppr_connector = PPRConnector()
                pattern_detector = MPOCryptoMLPatternDetector()
                normalizer = MPOCryptoMLNormalizer()
                
                # PPR
                ppr_result = ppr_connector.calculate_multi_source_ppr(contract, graph, auto_detect_sources=True)
                ml_features["ppr_score"] = ppr_result.get("total_ppr", 0.0)
                ml_features["sdn_ppr"] = ppr_result.get("sdn_ppr", 0.0)
                ml_features["mixer_ppr"] = ppr_result.get("mixer_ppr", 0.0)

                # Patterns
                pattern_detector._build_graph()  # ì´ˆê¸°í™”
                for tx in transactions:
                    pattern_detector.add_transaction(tx)
                
                # contract ì£¼ì†Œê°€ ê·¸ë˜í”„ì— ìˆëŠ”ì§€ í™•ì¸
                contract_lower = contract.lower()
                contract_in_graph = contract_lower in pattern_detector.graph.nodes() if pattern_detector.graph else False
                
                # íŒ¨í„´ íƒì§€ ë° ì ìˆ˜ ê³„ì‚°
                if contract_in_graph:
                    # contract ì£¼ì†Œê°€ ê·¸ë˜í”„ì— ìˆìœ¼ë©´ í•´ë‹¹ ì£¼ì†Œ ì‚¬ìš©
                    target_address = contract_lower
                else:
                    # contract ì£¼ì†Œê°€ ê·¸ë˜í”„ì— ì—†ìœ¼ë©´ ê·¸ë˜í”„ì˜ ëª¨ë“  ë…¸ë“œì— ëŒ€í•œ í†µê³„ ì§‘ê³„
                    # ê°€ì¥ ë§ì´ ì—°ê²°ëœ ë…¸ë“œë¥¼ ëŒ€í‘œ ì£¼ì†Œë¡œ ì‚¬ìš©
                    if pattern_detector.graph and len(pattern_detector.graph.nodes()) > 0:
                        # in_degree + out_degreeê°€ ê°€ì¥ í° ë…¸ë“œ ì„ íƒ
                        max_degree = -1
                        target_address = None
                        for node in pattern_detector.graph.nodes():
                            degree = pattern_detector.graph.in_degree(node) + pattern_detector.graph.out_degree(node)
                            if degree > max_degree:
                                max_degree = degree
                                target_address = node
                        if target_address is None:
                            target_address = list(pattern_detector.graph.nodes())[0]
                    else:
                        target_address = contract_lower
                
                fan_in = pattern_detector.detect_fan_in_pattern(target_address)
                fan_out = pattern_detector.detect_fan_out_pattern(target_address)
                gather_scatter_value = pattern_detector.gather_scatter(target_address)
                gather_scatter_count = pattern_detector.gather_scatter_count(target_address)
                stack_paths = pattern_detector.detect_stack_pattern(target_address)  # ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                bipartite = pattern_detector.detect_bipartite_pattern([target_address])
                
                # ê·¸ë˜í”„ í†µê³„ Feature ì¶”ê°€
                # Fan-in/out ê°œìˆ˜ ë° ê°’ (í•­ìƒ ê³„ì‚°, contractê°€ ê·¸ë˜í”„ì— ì—†ì–´ë„ ëŒ€í‘œ ì£¼ì†Œ ì‚¬ìš©)
                ml_features["fan_in_count"] = fan_in.get("fan_in_count", 0)
                ml_features["fan_out_count"] = fan_out.get("fan_out_count", 0)
                ml_features["fan_in_value"] = fan_in.get("total_value", 0.0)
                ml_features["fan_out_value"] = fan_out.get("total_value", 0.0)
                
                # ì¶”ê°€: ê·¸ë˜í”„ ì „ì²´ í†µê³„ (contractì™€ ë¬´ê´€í•˜ê²Œ)
                if pattern_detector.graph:
                    # ê·¸ë˜í”„ì˜ ëª¨ë“  ë…¸ë“œì— ëŒ€í•œ fan-in/out ì§‘ê³„
                    total_fan_in_count = sum(pattern_detector.graph.in_degree(n) for n in pattern_detector.graph.nodes())
                    total_fan_out_count = sum(pattern_detector.graph.out_degree(n) for n in pattern_detector.graph.nodes())
                    total_fan_in_value = sum(pattern_detector.fan_in(n) for n in pattern_detector.graph.nodes())
                    total_fan_out_value = sum(pattern_detector.fan_out(n) for n in pattern_detector.graph.nodes())
                    
                    # í‰ê· ê°’ìœ¼ë¡œ ì •ê·œí™”
                    num_nodes = len(pattern_detector.graph.nodes())
                    if num_nodes > 0:
                        ml_features["avg_fan_in_count"] = total_fan_in_count / num_nodes
                        ml_features["avg_fan_out_count"] = total_fan_out_count / num_nodes
                        ml_features["avg_fan_in_value"] = total_fan_in_value / num_nodes
                        ml_features["avg_fan_out_value"] = total_fan_out_value / num_nodes
                    else:
                        ml_features["avg_fan_in_count"] = 0.0
                        ml_features["avg_fan_out_count"] = 0.0
                        ml_features["avg_fan_in_value"] = 0.0
                        ml_features["avg_fan_out_value"] = 0.0
                else:
                    ml_features["avg_fan_in_count"] = 0.0
                    ml_features["avg_fan_out_count"] = 0.0
                    ml_features["avg_fan_in_value"] = 0.0
                    ml_features["avg_fan_out_value"] = 0.0
                
                # íŒ¨í„´ ì ìˆ˜ ê³„ì‚°
                pattern_score = 0.0
                detected_patterns = []
                if fan_in.get("is_detected", False):
                    pattern_score += 10.0
                    detected_patterns.append("fan_in")
                if fan_out.get("is_detected", False):
                    pattern_score += 10.0
                    detected_patterns.append("fan_out")
                # Gather-scatter: fan_inê³¼ fan_outì´ ë™ì‹œì— ìˆê±°ë‚˜, gather_scatter ê°’ ìì²´ê°€ ì„ê³„ê°’ ì´ìƒ
                if (fan_in.get("is_detected", False) and 
                    fan_out.get("is_detected", False)) or \
                   (gather_scatter_value > 0 and gather_scatter_count >= 5):
                    pattern_score += 10.0
                    detected_patterns.append("gather_scatter")
                if isinstance(stack_paths, list) and len(stack_paths) > 0:
                    pattern_score += 10.0
                    detected_patterns.append("stack")
                if bipartite.get("is_bipartite", False):
                    pattern_score += 10.0
                    detected_patterns.append("bipartite")
                
                ml_features["pattern_score"] = pattern_score
                ml_features["fan_in_detected"] = 1 if "fan_in" in detected_patterns else 0
                ml_features["fan_out_detected"] = 1 if "fan_out" in detected_patterns else 0
                ml_features["gather_scatter_detected"] = 1 if "gather_scatter" in detected_patterns else 0
                ml_features["stack_detected"] = 1 if "stack" in detected_patterns else 0
                ml_features["bipartite_detected"] = 1 if "bipartite" in detected_patterns else 0
                
                # ê±°ë˜ ê¸ˆì•¡ í†µê³„ Feature ì¶”ê°€
                import numpy as np
                transaction_values = []
                for tx in transactions:
                    value = tx.get("usd_value", 0)
                    if value > 0:
                        transaction_values.append(value)
                
                if transaction_values:
                    ml_features["avg_transaction_value"] = float(np.mean(transaction_values))
                    ml_features["max_transaction_value"] = float(np.max(transaction_values))
                    ml_features["min_transaction_value"] = float(np.min(transaction_values))
                    ml_features["total_transaction_value"] = float(np.sum(transaction_values))
                    ml_features["transaction_count"] = len(transaction_values)
                else:
                    # USD ê°’ì´ ì—†ìœ¼ë©´ Wei ê°’ ì‚¬ìš© (ETHë¡œ ë³€í™˜)
                    wei_values = []
                    for tx in transactions:
                        wei_value = tx.get("value", 0)
                        if wei_value > 0:
                            eth_value = wei_value / 1e18
                            wei_values.append(eth_value)
                    
                    if wei_values:
                        ml_features["avg_transaction_value"] = float(np.mean(wei_values))
                        ml_features["max_transaction_value"] = float(np.max(wei_values))
                        ml_features["min_transaction_value"] = float(np.min(wei_values))
                        ml_features["total_transaction_value"] = float(np.sum(wei_values))
                        ml_features["transaction_count"] = len(wei_values)
                    else:
                        ml_features["avg_transaction_value"] = 0.0
                        ml_features["max_transaction_value"] = 0.0
                        ml_features["min_transaction_value"] = 0.0
                        ml_features["total_transaction_value"] = 0.0
                        ml_features["transaction_count"] = 0

                # NTS, NWS
                n_theta = normalizer.normalize_timestamp(contract, graph, transactions)
                n_omega = normalizer.normalize_weight(contract, graph, transactions)
                ml_features["n_theta"] = n_theta
                ml_features["n_omega"] = n_omega
            
            # ê° ê±°ë˜ì— ëŒ€í•´ ë°ì´í„°ì…‹ ì¶”ê°€ (ë£° í‰ê°€ ê²°ê³¼ ì œê±°)
            for tx in transactions:
                # ë£° í‰ê°€ëŠ” í•™ìŠµ ì‹œì—ë§Œ ìˆ˜í–‰ (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)
                # ì—¬ê¸°ì„œëŠ” ë£° í‰ê°€ ê²°ê³¼ë¥¼ ìˆ˜ì§‘ë§Œ í•¨ (í†µê³„ìš©)
                tx_for_eval = builder._convert_transaction(tx)
                rule_results = builder.rule_evaluator.evaluate_single_transaction(tx_for_eval)
                
                # ë°œë™ëœ ë£° ID ìˆ˜ì§‘ (í†µê³„ìš©)
                for rule in rule_results:
                    rule_counter[rule.get("rule_id")] += 1
                
                # ë°ì´í„° í–¥ìƒ ì—¬ë¶€ í™•ì¸
                if tx.get("is_sanctioned") or tx.get("is_mixer") or tx.get("usd_value", 0) > 0:
                    enhanced_count += 1
                
                actual_score = builder._label_to_score(label)
                
                # ê° ê±°ë˜ë³„ fan-in/out í†µê³„ ê³„ì‚° (ê±°ë˜ì˜ from/to ì£¼ì†Œ ê¸°ì¤€)
                tx_ml_features = ml_features.copy()  # ê¸°ë³¸ ML í”¼ì²˜ ë³µì‚¬
                
                if pattern_detector.graph:
                    from_addr = tx.get("from", "").lower()
                    to_addr = tx.get("to", "").lower()
                    
                    # from ì£¼ì†Œ ê¸°ì¤€ fan-out ê³„ì‚°
                    if from_addr in pattern_detector.graph.nodes():
                        from_fan_out = pattern_detector.detect_fan_out_pattern(from_addr)
                        tx_ml_features["tx_from_fan_out_count"] = from_fan_out.get("fan_out_count", 0)
                        tx_ml_features["tx_from_fan_out_value"] = from_fan_out.get("total_value", 0.0)
                    else:
                        tx_ml_features["tx_from_fan_out_count"] = 0
                        tx_ml_features["tx_from_fan_out_value"] = 0.0
                    
                    # to ì£¼ì†Œ ê¸°ì¤€ fan-in ê³„ì‚°
                    if to_addr in pattern_detector.graph.nodes():
                        to_fan_in = pattern_detector.detect_fan_in_pattern(to_addr)
                        tx_ml_features["tx_to_fan_in_count"] = to_fan_in.get("fan_in_count", 0)
                        tx_ml_features["tx_to_fan_in_value"] = to_fan_in.get("total_value", 0.0)
                    else:
                        tx_ml_features["tx_to_fan_in_count"] = 0
                        tx_ml_features["tx_to_fan_in_value"] = 0.0
                    
                    # ê±°ë˜ ë°©í–¥ì„±: from -> to
                    # ê±°ë˜ì˜ ì£¼ì²´ ì£¼ì†Œë¥¼ ê²°ì • (from ë˜ëŠ” to ì¤‘ ë” ë§ì€ ì—°ê²°ì„ ê°€ì§„ ì£¼ì†Œ)
                    if from_addr in pattern_detector.graph.nodes() and to_addr in pattern_detector.graph.nodes():
                        from_degree = pattern_detector.graph.out_degree(from_addr) + pattern_detector.graph.in_degree(from_addr)
                        to_degree = pattern_detector.graph.out_degree(to_addr) + pattern_detector.graph.in_degree(to_addr)
                        primary_address = from_addr if from_degree >= to_degree else to_addr
                    elif from_addr in pattern_detector.graph.nodes():
                        primary_address = from_addr
                    elif to_addr in pattern_detector.graph.nodes():
                        primary_address = to_addr
                    else:
                        primary_address = None
                    
                    if primary_address:
                        primary_fan_in = pattern_detector.detect_fan_in_pattern(primary_address)
                        primary_fan_out = pattern_detector.detect_fan_out_pattern(primary_address)
                        tx_ml_features["tx_primary_fan_in_count"] = primary_fan_in.get("fan_in_count", 0)
                        tx_ml_features["tx_primary_fan_in_value"] = primary_fan_in.get("total_value", 0.0)
                        tx_ml_features["tx_primary_fan_out_count"] = primary_fan_out.get("fan_out_count", 0)
                        tx_ml_features["tx_primary_fan_out_value"] = primary_fan_out.get("total_value", 0.0)
                    else:
                        tx_ml_features["tx_primary_fan_in_count"] = 0
                        tx_ml_features["tx_primary_fan_in_value"] = 0.0
                        tx_ml_features["tx_primary_fan_out_count"] = 0
                        tx_ml_features["tx_primary_fan_out_value"] = 0.0
                else:
                    tx_ml_features["tx_from_fan_out_count"] = 0
                    tx_ml_features["tx_from_fan_out_value"] = 0.0
                    tx_ml_features["tx_to_fan_in_count"] = 0
                    tx_ml_features["tx_to_fan_in_value"] = 0.0
                    tx_ml_features["tx_primary_fan_in_count"] = 0
                    tx_ml_features["tx_primary_fan_in_value"] = 0.0
                    tx_ml_features["tx_primary_fan_out_count"] = 0
                    tx_ml_features["tx_primary_fan_out_value"] = 0.0
                
                # ê±°ë˜ ì»¨í…ìŠ¤íŠ¸
                tx_context = {
                    "amount_usd": tx.get("usd_value", 0),
                    "is_sanctioned": tx.get("is_sanctioned", False),
                    "is_mixer": tx.get("is_mixer", False),
                    "chain": chain,
                    "num_transactions": len(transactions),
                    "graph_nodes": graph.number_of_nodes(),
                    "graph_edges": graph.number_of_edges(),
                }
                
                # ë°ì´í„°ì…‹ ì¶”ê°€ (rule_results, rule_score ì œê±°)
                dataset.append({
                    # ì›ë³¸ íŠ¸ëœì­ì…˜ ë°ì´í„°
                    "tx_hash": tx.get("tx_hash", ""),
                    "from": tx.get("from", ""),
                    "to": tx.get("to", ""),
                    "timestamp": tx.get("timestamp", 0),
                    "usd_value": tx.get("usd_value", 0),
                    "value": tx.get("value", 0),  # ì›ë³¸ Wei ê°’
                    "chain": chain,
                    "block_height": tx.get("block_height", 0),
                    
                    # ê±°ë˜ ì»¨í…ìŠ¤íŠ¸
                    "tx_context": tx_context,
                    
                    # ML í”¼ì²˜ (ê±°ë˜ë³„ í†µê³„ í¬í•¨)
                    "ml_features": tx_ml_features,
                    
                    # Ground truth
                    "ground_truth_label": "fraud" if label == 1 else "normal",
                    "actual_risk_score": actual_score,
                    
                    # ë©”íƒ€ë°ì´í„°
                    "address": contract,
                    "data_source": "legacy_enhanced"
                    
                    # âŒ rule_results ì œê±° (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)
                    # âŒ rule_score ì œê±° (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€)
                })
        
        except Exception as e:
            print(f"\nâš ï¸  ì—ëŸ¬ ({contract}): {e}")
            continue
    
    print("\n" + "=" * 80)
    print("âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
    print("=" * 80)
    print(f"\nğŸ“Š í†µê³„:")
    print(f"   ì´ ìƒ˜í”Œ: {len(dataset)}ê°œ")
    if len(dataset) > 0:
        print(f"   í–¥ìƒëœ ìƒ˜í”Œ: {enhanced_count}ê°œ ({enhanced_count/len(dataset)*100:.1f}%)")
    else:
        print(f"   í–¥ìƒëœ ìƒ˜í”Œ: {enhanced_count}ê°œ")
    
    print(f"\nğŸ“ˆ ë°œë™ëœ ë£° ë¶„í¬:")
    for rule_id, count in rule_counter.most_common(10):
        print(f"   {rule_id}: {count}íšŒ ({count/len(dataset)*100:.1f}%)")
    
    # ì €ì¥
    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w') as f:
        json.dump(dataset, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ì €ì¥ ìœ„ì¹˜: {output_path}")
    print(f"   íŒŒì¼ í¬ê¸°: {output_file.stat().st_size / 1024 / 1024:.2f} MB")
    
    return dataset


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ë‹¤ì–‘í•œ ë£° ë°œë™ì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘")
    parser.add_argument(
        "--features-path",
        type=str,
        default="legacy/data/features/ethereum_basic_metrics_processed.csv",
        help="Features CSV íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--transactions-dir",
        type=str,
        default="legacy/data/transactions",
        help="ê±°ë˜ ë°ì´í„° ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        "--output-path",
        type=str,
        default="data/dataset/diverse_rules.json",
        help="ì¶œë ¥ JSON íŒŒì¼ ê²½ë¡œ"
    )
    parser.add_argument(
        "--max-txs-per-contract",
        type=int,
        default=100,
        help="ì£¼ì†Œë‹¹ ìµœëŒ€ ê±°ë˜ ìˆ˜"
    )
    parser.add_argument(
        "--sample-ratio",
        type=float,
        default=1.0,
        help="ìƒ˜í”Œë§ ë¹„ìœ¨ (0.0 ~ 1.0)"
    )
    parser.add_argument(
        "--use-etherscan",
        action="store_true",
        help="Etherscan API ì‚¬ìš© (Rate limit ì£¼ì˜)"
    )
    
    args = parser.parse_args()
    
    dataset = collect_diverse_rules_data(
        features_path=args.features_path,
        transactions_dir=args.transactions_dir,
        output_path=args.output_path,
        max_transactions_per_contract=args.max_txs_per_contract,
        sample_ratio=args.sample_ratio,
        use_etherscan=args.use_etherscan
    )
    
    print("\nâœ… ì™„ë£Œ!")
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("1. ë°ì´í„°ì…‹ ë¶„í• : python scripts/split_dataset.py")
    print("2. Rule-based ëª¨ë¸ ì¬í•™ìŠµ: python scripts/optimize_rule_based.py")


if __name__ == "__main__":
    main()

